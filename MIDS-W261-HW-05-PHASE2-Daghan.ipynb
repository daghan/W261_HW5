{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5 Phase 2\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "__Week:__   5.5\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Thursday of the following week by 8AM (West coast time).\n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as Altiscale's PaaS or on AWS) and is due Thursday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the Altiscale cluster and will be due Thursday of the following week by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "For Phase 2 you will first use the small datasets from phase 1 to systems test your code in the cloud. Then you will test your code on 1 file and then 20 files before running the full (191 file) Google n-gram dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Small data for systems tests__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Paths to Main data in HDFS on Altiscale__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = \"/user/winegarj/data/1_test\"\n",
    "TEST_20 = \"/user/winegarj/data/20_test\"\n",
    "FULL_DATA = \"/user/winegarj/data/full\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set - Up for Phase 2\n",
    "Before you can run your simlarity analysis on the full Google n-gram dataset you should confirm that the code your wrote in Phase 1 works on the cloud. In the space below, copy the code for your three jobs from Phase 1 (`buildStripes.py`, `invertedIndex.py`, `similarity.py`) and rerun your  atlas-boon systems tests on Altiscale (i.e. ** the cloud**). NOTE: _you may end up modifying this code when you get to 5.7, that's fine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `buildStripes.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol # Not strictly necessary, but the output is prettier this way\n",
    "\n",
    "\n",
    "    \n",
    "    stripe = defaultdict(int)\n",
    "    current_word = None\n",
    "    total_count = 0\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        words, count, pages_count, books_count = line.strip().split(\"\\t\")\n",
    "        words = words.lower().split(\" \")\n",
    "        self.stripe = defaultdict(int)\n",
    "        for word1 in words:\n",
    "            for word2 in words:\n",
    "                if (word1 != word2):\n",
    "                    self.stripe[word2] = int(count)\n",
    "            final_string = \"\"\n",
    "            for word3, count3 in self.stripe.iteritems():\n",
    "                final_string +=  \"\\t\" + word3 + \":\" + str(count3)\n",
    "            yield word1, final_string\n",
    "        \n",
    "\n",
    "    def reducer(self, word, lines):\n",
    "        if (word == self.current_word):\n",
    "            for line in lines:\n",
    "                couples = line.strip().split(\"\\t\")\n",
    "                for couple in couples:\n",
    "                    word2, count = couple.split(\":\")\n",
    "                    if (word2 != word):\n",
    "                        self.stripe[word2] += int(count)\n",
    "        else:\n",
    "            if (self.current_word):\n",
    "                yield self.current_word, self.stripe\n",
    "            self.current_word = word\n",
    "            self.stripe = defaultdict(int)\n",
    "            for line in lines:\n",
    "                couples = line.strip().split(\"\\t\")\n",
    "                for couple in couples:\n",
    "                    word2, count = couple.split(\":\")\n",
    "                    if (word2 != word):\n",
    "                        self.stripe[word2] += int(count)   \n",
    " \n",
    "            \n",
    "    def reducer_final(self):\n",
    "        if (self.current_word):\n",
    "            yield self.current_word, self.stripe\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                    mapper = self.mapper,\n",
    "                    reducer = self.reducer,\n",
    "                    reducer_final = self.reducer_final)]\n",
    "    \n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `invertedIndex.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "  #START SUDENT CODE531_INV_INDEX\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    words = []\n",
    "    cur_word = None\n",
    "    cur_count = 0\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        word1, stripe = line.strip().split('\\t')\n",
    "        word1 = word1.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripe)\n",
    "        for word2 in stripe:\n",
    "            yield (word2,word1), len(stripe)\n",
    "\n",
    "        \n",
    "\n",
    "    def reducer(self, words, count):\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        cur_count = [c for c in count][0]\n",
    "        if (word1 == self.cur_word):\n",
    "            #same word, add to the list\n",
    "            self.words.append((word2,cur_count))\n",
    "        else:\n",
    "            # it is a new word\n",
    "            if (self.cur_word):\n",
    "                yield self.cur_word, self.words\n",
    "            self.cur_word = word1\n",
    "            self.words = [(word2,cur_count)]\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        if (self.cur_word):\n",
    "             yield self.cur_word, self.words\n",
    "        \n",
    "    def steps(self):\n",
    "      \n",
    "        return [MRStep(\n",
    "                    mapper = self.mapper,\n",
    "                    reducer = self.reducer,\n",
    "                    reducer_final = self.reducer_final)]\n",
    "\n",
    "  #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `similarity.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "  #START SUDENT CODE531_SIMILARITY\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, stripe = line.split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "        for couple_pair in itertools.combinations(stripe, 2):\n",
    "            (word1,card1),(word2,card2) = couple_pair\n",
    "            yield (word1,word2), (1,card1,card2)\n",
    "        \n",
    "        \n",
    "    def reducer(self, words, counts):\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        cur_count = 0\n",
    "        for count in counts:\n",
    "            card1 = int(count[1])\n",
    "            card2 = int(count[2])\n",
    "            cur_count += int(count[0])\n",
    "        jaccard = cur_count / (card1 + card2 - cur_count)\n",
    "        cosine = cur_count / np.sqrt(card1 * card2)\n",
    "        avg = (cosine + jaccard) / 2\n",
    "        yield float(avg), ((word1,word2), cosine, jaccard)\n",
    "        \n",
    "    def reducer1(self, avg, lines):\n",
    "        for line in lines:\n",
    "            yield avg, line\n",
    "        \n",
    "    \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep( mapper = self.mapper,\n",
    "                    reducer = self.reducer),\n",
    "            MRStep( jobconf = \n",
    "                   {'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'stream.map.output.field.separator':'\\t',\n",
    "                    'mapreduce.partition.keycomparator.options': '-k1,1nr -k2,2',\n",
    "                    'mapreduce.job.reduces': '1'\n",
    "                    },\n",
    "                    reducer = self.reducer1)\n",
    "                ]\n",
    "          \n",
    "  #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atlas-boon systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 05:41:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/atlas-boon/stripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes.daghan.20180308.054151.784062\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/buildStripes.daghan.20180308.054151.784062/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob967761004224666468.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30416\n",
      "  Submitted application application_1509050304403_30416\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30416/\n",
      "  Running job: job_1509050304403_30416\n",
      "  Job job_1509050304403_30416 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30416 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/atlas-boon/stripes\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=154\n",
      "\t\tFILE: Number of bytes written=394382\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=475\n",
      "\t\tHDFS: Number of bytes written=163\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=79641600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=73940480\n",
      "\t\tTotal time spent by all map tasks (ms)=51850\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=155550\n",
      "\t\tTotal time spent by all reduce tasks (ms)=28883\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=144415\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51850\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=28883\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4260\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1085\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=214\n",
      "\t\tMap output materialized bytes=185\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1891835904\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=185\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=2264924160\n",
      "\t\tVirtual memory (bytes) snapshot=11411681280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/buildStripes.daghan.20180308.054151.784062...\n",
      "Removing temp directory /tmp/buildStripes.daghan.20180308.054151.784062...\n"
     ]
    }
   ],
   "source": [
    "# ADD CELLS HERE\n",
    "OUTPUT_PATH = \"/user/daghan/hw5/atlas-boon/stripes\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt --output-dir={OUTPUT_PATH} --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\r\n",
      "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\r\n",
      "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\r\n",
      "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_stripes_2\n",
    "!cat system_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 02:33:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/atlas-boon/index' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex.daghan.20180307.023327.183773\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180307.023327.183773/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8843711676065664246.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_26857\n",
      "  Submitted application application_1509050304403_26857\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_26857/\n",
      "  Running job: job_1509050304403_26857\n",
      "  Job job_1509050304403_26857 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_26857 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/atlas-boon/index\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=245\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=173\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=138\n",
      "\t\tFILE: Number of bytes written=395854\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=611\n",
      "\t\tHDFS: Number of bytes written=173\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19849728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8581120\n",
      "\t\tTotal time spent by all map tasks (ms)=12923\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38769\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3352\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16760\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12923\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3352\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2490\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=311\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=216\n",
      "\t\tMap output materialized bytes=200\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1889259520\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=2361393152\n",
      "\t\tVirtual memory (bytes) snapshot=11409727488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180307.023327.183773...\n",
      "Removing temp directory /tmp/invertedIndex.daghan.20180307.023327.183773...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/atlas-boon/index\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python invertedIndex.py -r hadoop systems_test_stripes_2 --output-dir={OUTPUT_PATH} --no-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t[[\"boon\", 3], [\"dipped\", 3]]\r\n",
      "\"boon\"\t[[\"atlas\", 2], [\"cava\", 2], [\"dipped\", 3]]\r\n",
      "\"cava\"\t[[\"boon\", 3], [\"dipped\", 3]]\r\n",
      "\"dipped\"\t[[\"atlas\", 2], [\"boon\", 3], [\"cava\", 2]]\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_index_2\n",
    "!cat systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 05:47:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/atlas-boon/similarities' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity.daghan.20180308.054709.340806\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.054709.340806/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob630185576068367768.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30428\n",
      "  Submitted application application_1509050304403_30428\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30428/\n",
      "  Running job: job_1509050304403_30428\n",
      "  Job job_1509050304403_30428 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30428 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.054709.340806/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=260\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=370\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=135\n",
      "\t\tFILE: Number of bytes written=395862\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=614\n",
      "\t\tHDFS: Number of bytes written=370\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=66186240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16176640\n",
      "\t\tTotal time spent by all map tasks (ms)=43090\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=129270\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6319\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31595\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=43090\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6319\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=886\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=236\n",
      "\t\tMap output materialized bytes=205\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1899618304\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=205\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=2259156992\n",
      "\t\tVirtual memory (bytes) snapshot=11414040576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3868823935942055862.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30432\n",
      "  Submitted application application_1509050304403_30432\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30432/\n",
      "  Running job: job_1509050304403_30432\n",
      "  Job job_1509050304403_30432 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30432 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/atlas-boon/similarities\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=555\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=365\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=200\n",
      "\t\tFILE: Number of bytes written=397031\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=911\n",
      "\t\tHDFS: Number of bytes written=365\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18835968\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=26531840\n",
      "\t\tTotal time spent by all map tasks (ms)=12263\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36789\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10364\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=51820\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12263\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10364\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2980\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=470\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=6\n",
      "\t\tMap output bytes=376\n",
      "\t\tMap output materialized bytes=268\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1887199232\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=268\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=2261254144\n",
      "\t\tVirtual memory (bytes) snapshot=11409534976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.054709.340806...\n",
      "Removing temp directory /tmp/similarity.daghan.20180308.054709.340806...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/atlas-boon/similarities\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python similarity.py -r hadoop systems_test_index_2 --output-dir={OUTPUT_PATH} --no-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[[\"atlas\", \"cava\"], 1.0, 1.0]\r\n",
      "0.5833333333333333\t[[\"boon\", \"dipped\"], 0.6666666666666666, 0.5]\r\n",
      "0.32912414523193156\t[[\"atlas\", \"boon\"], 0.4082482904638631, 0.25]\r\n",
      "0.32912414523193156\t[[\"atlas\", \"dipped\"], 0.4082482904638631, 0.25]\r\n",
      "0.32912414523193156\t[[\"boon\", \"cava\"], 0.4082482904638631, 0.25]\r\n",
      "0.32912414523193156\t[[\"cava\", \"dipped\"], 0.4082482904638631, 0.25]\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_similarities_2\n",
    "!cat systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-line systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 02:35:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/10-line/stripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes.daghan.20180307.023537.832946\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/buildStripes.daghan.20180307.023537.832946/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7403578810488008410.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_26864\n",
      "  Submitted application application_1509050304403_26864\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_26864/\n",
      "  Running job: job_1509050304403_26864\n",
      "  Job job_1509050304403_26864 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_26864 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/10-line/stripes\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2407\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=924\n",
      "\t\tFILE: Number of bytes written=396029\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1011\n",
      "\t\tHDFS: Number of bytes written=2407\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15820800\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15278080\n",
      "\t\tTotal time spent by all map tasks (ms)=10300\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30900\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5968\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29840\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10300\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5968\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2630\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=264\n",
      "\t\tInput split bytes=448\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=2926\n",
      "\t\tMap output materialized bytes=957\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896538112\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=50\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=957\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=100\n",
      "\t\tTotal committed heap usage (bytes)=2266497024\n",
      "\t\tVirtual memory (bytes) snapshot=11414503424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/buildStripes.daghan.20180307.023537.832946...\n",
      "Removing temp directory /tmp/buildStripes.daghan.20180307.023537.832946...\n"
     ]
    }
   ],
   "source": [
    "# ADD CELLS HERE\n",
    "OUTPUT_PATH = \"/user/daghan/hw5/10-line/stripes\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python buildStripes.py -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --output-dir={OUTPUT_PATH} --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\": 55, \"female\": 447, \"general\": 92, \"sea\": 62, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"government\": 102, \"the\": 124, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"child's\": 1099, \"collection\": 239, \"by\": 62, \"case\": 604, \"circumstantial\": 62, \"of\": 895, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"fairy\": 123}\r\n",
      "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\r\n",
      "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\r\n",
      "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\r\n",
      "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\r\n",
      "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\r\n",
      "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"child's\": 1099, \"in\": 1099}\r\n",
      "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\r\n",
      "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\r\n",
      "\"collection\"\t{\"forms\": 116, \"of\": 239, \"fairy\": 123, \"a\": 239, \"tales\": 123}\r\n",
      "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\r\n",
      "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\r\n",
      "\"female\"\t{\"case\": 447, \"of\": 447, \"study\": 447, \"a\": 447}\r\n",
      "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\r\n",
      "\"forms\"\t{\"a\": 116, \"of\": 116, \"collection\": 116}\r\n",
      "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\r\n",
      "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\r\n",
      "\"government\"\t{\"case\": 102, \"a\": 102, \"study\": 102, \"in\": 102}\r\n",
      "\"in\"\t{\"case\": 102, \"a\": 1201, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\r\n",
      "\"limited\"\t{\"case\": 55, \"of\": 55, \"study\": 55, \"a\": 55}\r\n",
      "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\r\n",
      "\"of\"\t{\"a\": 1011, \"case\": 502, \"circumstantial\": 62, \"limited\": 55, \"tales\": 123, \"collection\": 355, \"george\": 92, \"forms\": 232, \"female\": 447, \"narrative\": 62, \"study\": 502, \"fairy\": 123, \"general\": 92, \"the\": 62, \"biography\": 92}\r\n",
      "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\r\n",
      "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\r\n",
      "\"study\"\t{\"case\": 604, \"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\r\n",
      "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\r\n",
      "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\r\n",
      "\"wales\"\t{\"a\": 1099, \"child's\": 1099, \"christmas\": 1099, \"in\": 1099}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_stripes_1\n",
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 02:36:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/10-line/index' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex.daghan.20180307.023626.664082\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180307.023626.664082/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1937769588624129150.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_26866\n",
      "  Submitted application application_1509050304403_26866\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_26866/\n",
      "  Running job: job_1509050304403_26866\n",
      "  Job job_1509050304403_26866 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_26866 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/10-line/index\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3611\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2508\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1246\n",
      "\t\tFILE: Number of bytes written=398265\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3977\n",
      "\t\tHDFS: Number of bytes written=2508\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=22743552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=21683200\n",
      "\t\tTotal time spent by all map tasks (ms)=14807\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44421\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8470\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=42350\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14807\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8470\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2890\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=320\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3466\n",
      "\t\tMap output materialized bytes=1512\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1895206912\n",
      "\t\tReduce input groups=158\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1512\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=2174746624\n",
      "\t\tVirtual memory (bytes) snapshot=11406835712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180307.023626.664082...\n",
      "Removing temp directory /tmp/invertedIndex.daghan.20180307.023626.664082...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/10-line/index\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python invertedIndex.py -r hadoop systems_test_stripes_1 --output-dir={OUTPUT_PATH} --no-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t[[\"bill\", 4], [\"biography\", 4], [\"by\", 4], [\"case\", 7], [\"child's\", 4], [\"christmas\", 4], [\"circumstantial\", 4], [\"city\", 4], [\"collection\", 5], [\"establishing\", 4], [\"fairy\", 4], [\"female\", 4], [\"for\", 4], [\"forms\", 3], [\"general\", 4], [\"george\", 4], [\"government\", 4], [\"in\", 7], [\"limited\", 4], [\"narrative\", 4], [\"of\", 15], [\"religious\", 4], [\"sea\", 4], [\"study\", 7], [\"tales\", 4], [\"the\", 7], [\"wales\", 4]]\r\n",
      "\"bill\"\t[[\"a\", 27], [\"establishing\", 4], [\"for\", 4], [\"religious\", 4]]\r\n",
      "\"biography\"\t[[\"a\", 27], [\"general\", 4], [\"george\", 4], [\"of\", 15]]\r\n",
      "\"by\"\t[[\"a\", 27], [\"city\", 4], [\"sea\", 4], [\"the\", 7]]\r\n",
      "\"case\"\t[[\"a\", 27], [\"female\", 4], [\"government\", 4], [\"in\", 7], [\"limited\", 4], [\"of\", 15], [\"study\", 7]]\r\n",
      "\"child's\"\t[[\"a\", 27], [\"christmas\", 4], [\"in\", 7], [\"wales\", 4]]\r\n",
      "\"christmas\"\t[[\"a\", 27], [\"child's\", 4], [\"in\", 7], [\"wales\", 4]]\r\n",
      "\"circumstantial\"\t[[\"a\", 27], [\"narrative\", 4], [\"of\", 15], [\"the\", 7]]\r\n",
      "\"city\"\t[[\"a\", 27], [\"by\", 4], [\"sea\", 4], [\"the\", 7]]\r\n",
      "\"collection\"\t[[\"a\", 27], [\"fairy\", 4], [\"forms\", 3], [\"of\", 15], [\"tales\", 4]]\r\n",
      "\"establishing\"\t[[\"a\", 27], [\"bill\", 4], [\"for\", 4], [\"religious\", 4]]\r\n",
      "\"fairy\"\t[[\"a\", 27], [\"collection\", 5], [\"of\", 15], [\"tales\", 4]]\r\n",
      "\"female\"\t[[\"a\", 27], [\"case\", 7], [\"of\", 15], [\"study\", 7]]\r\n",
      "\"for\"\t[[\"a\", 27], [\"bill\", 4], [\"establishing\", 4], [\"religious\", 4]]\r\n",
      "\"forms\"\t[[\"a\", 27], [\"collection\", 5], [\"of\", 15]]\r\n",
      "\"general\"\t[[\"a\", 27], [\"biography\", 4], [\"george\", 4], [\"of\", 15]]\r\n",
      "\"george\"\t[[\"a\", 27], [\"biography\", 4], [\"general\", 4], [\"of\", 15]]\r\n",
      "\"government\"\t[[\"a\", 27], [\"case\", 7], [\"in\", 7], [\"study\", 7]]\r\n",
      "\"in\"\t[[\"a\", 27], [\"case\", 7], [\"child's\", 4], [\"christmas\", 4], [\"government\", 4], [\"study\", 7], [\"wales\", 4]]\r\n",
      "\"limited\"\t[[\"a\", 27], [\"case\", 7], [\"of\", 15], [\"study\", 7]]\r\n",
      "\"narrative\"\t[[\"a\", 27], [\"circumstantial\", 4], [\"of\", 15], [\"the\", 7]]\r\n",
      "\"of\"\t[[\"a\", 27], [\"biography\", 4], [\"case\", 7], [\"circumstantial\", 4], [\"collection\", 5], [\"fairy\", 4], [\"female\", 4], [\"forms\", 3], [\"general\", 4], [\"george\", 4], [\"limited\", 4], [\"narrative\", 4], [\"study\", 7], [\"tales\", 4], [\"the\", 7]]\r\n",
      "\"religious\"\t[[\"a\", 27], [\"bill\", 4], [\"establishing\", 4], [\"for\", 4]]\r\n",
      "\"sea\"\t[[\"a\", 27], [\"by\", 4], [\"city\", 4], [\"the\", 7]]\r\n",
      "\"study\"\t[[\"a\", 27], [\"case\", 7], [\"female\", 4], [\"government\", 4], [\"in\", 7], [\"limited\", 4], [\"of\", 15]]\r\n",
      "\"tales\"\t[[\"a\", 27], [\"collection\", 5], [\"fairy\", 4], [\"of\", 15]]\r\n",
      "\"the\"\t[[\"a\", 27], [\"by\", 4], [\"circumstantial\", 4], [\"city\", 4], [\"narrative\", 4], [\"of\", 15], [\"sea\", 4]]\r\n",
      "\"wales\"\t[[\"a\", 27], [\"child's\", 4], [\"christmas\", 4], [\"in\", 7]]\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_index_1\n",
    "!cat systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 02:37:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/10-line/similarities' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity.daghan.20180307.023722.436327\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180307.023722.436327/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8135191918721984236.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_26869\n",
      "  Submitted application application_1509050304403_26869\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_26869/\n",
      "  Running job: job_1509050304403_26869\n",
      "  Job job_1509050304403_26869 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_26869 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180307.023722.436327/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3762\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27286\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3886\n",
      "\t\tFILE: Number of bytes written=404969\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4116\n",
      "\t\tHDFS: Number of bytes written=27286\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11633664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16189440\n",
      "\t\tTotal time spent by all map tasks (ms)=7574\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22722\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6324\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31620\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7574\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6324\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3500\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=340\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=21258\n",
      "\t\tMap output materialized bytes=5096\n",
      "\t\tMap output records=673\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1923956736\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=673\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=5096\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1346\n",
      "\t\tTotal committed heap usage (bytes)=2272788480\n",
      "\t\tVirtual memory (bytes) snapshot=11415265280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4701725518302218503.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_26870\n",
      "  Submitted application application_1509050304403_26870\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_26870/\n",
      "  Running job: job_1509050304403_26870\n",
      "  Job job_1509050304403_26870 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1509050304403_26870 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/10-line/similarities\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=40929\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27259\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=263016\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=41285\n",
      "\t\tHDFS: Number of bytes written=27259\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17092608\n",
      "\t\tTotal time spent by all map tasks (ms)=11128\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33384\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11128\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1370\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=143\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=378\n",
      "\t\tMap output records=378\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=511721472\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=739246080\n",
      "\t\tVirtual memory (bytes) snapshot=6833389568\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180307.023722.436327...\n",
      "Removing temp directory /tmp/similarity.daghan.20180307.023722.436327...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/10-line/similarities\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python similarity.py -r hadoop systems_test_index_1 --output-dir={OUTPUT_PATH} --no-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.197908995868835\t[[\"a\", \"bill\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"biography\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"by\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.3253607473788495\t[[\"a\", \"case\"], 0.4364357804719847, 0.21428571428571427]\t\r\n",
      "0.197908995868835\t[[\"a\", \"child's\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"christmas\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"circumstantial\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"city\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.2435611645933455\t[[\"a\", \"collection\"], 0.34426518632954817, 0.14285714285714285]\t\r\n",
      "0.197908995868835\t[[\"a\", \"establishing\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"fairy\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"female\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"for\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.1468253968253968\t[[\"a\", \"forms\"], 0.2222222222222222, 0.07142857142857142]\t\r\n",
      "0.197908995868835\t[[\"a\", \"general\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"george\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"government\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.3253607473788495\t[[\"a\", \"in\"], 0.4364357804719847, 0.21428571428571427]\t\r\n",
      "0.197908995868835\t[[\"a\", \"limited\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"narrative\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.5978327964999672\t[[\"a\", \"of\"], 0.6956655929999346, 0.5]\t\r\n",
      "0.197908995868835\t[[\"a\", \"religious\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.197908995868835\t[[\"a\", \"sea\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.3253607473788495\t[[\"a\", \"study\"], 0.4364357804719847, 0.21428571428571427]\t\r\n",
      "0.197908995868835\t[[\"a\", \"tales\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.3253607473788495\t[[\"a\", \"the\"], 0.4364357804719847, 0.21428571428571427]\t\r\n",
      "0.197908995868835\t[[\"a\", \"wales\"], 0.28867513459481287, 0.10714285714285714]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"biography\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"by\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"bill\", \"case\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"child's\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"christmas\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"circumstantial\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"city\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.17430339887498947\t[[\"bill\", \"collection\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.675\t[[\"bill\", \"establishing\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.675\t[[\"bill\", \"for\"], 0.75, 0.6]\t\r\n",
      "0.2276709006307398\t[[\"bill\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"bill\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"bill\", \"of\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.675\t[[\"bill\", \"religious\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"bill\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"bill\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"bill\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"by\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"biography\", \"case\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"child's\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"christmas\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"circumstantial\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"city\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.36646394060712184\t[[\"biography\", \"collection\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"fairy\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"female\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.48867513459481293\t[[\"biography\", \"forms\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.675\t[[\"biography\", \"general\"], 0.75, 0.6]\t\r\n",
      "0.675\t[[\"biography\", \"george\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"biography\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"limited\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"biography\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"biography\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.41666666666666663\t[[\"biography\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"biography\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"biography\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"by\", \"case\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"child's\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"christmas\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"by\", \"circumstantial\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.675\t[[\"by\", \"city\"], 0.75, 0.6]\t\r\n",
      "0.17430339887498947\t[[\"by\", \"collection\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.2276709006307398\t[[\"by\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"by\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"by\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.18792297428534527\t[[\"by\", \"of\"], 0.2581988897471611, 0.11764705882352941]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.675\t[[\"by\", \"sea\"], 0.75, 0.6]\t\r\n",
      "0.14449111825230682\t[[\"by\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"by\", \"the\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"by\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"child's\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"christmas\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"circumstantial\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.14449111825230682\t[[\"case\", \"city\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.26903085094570334\t[[\"case\", \"collection\"], 0.3380617018914066, 0.2]\t\r\n",
      "0.14449111825230682\t[[\"case\", \"establishing\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"fairy\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.4709733547569204\t[[\"case\", \"female\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.14449111825230682\t[[\"case\", \"for\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.34321789023599236\t[[\"case\", \"forms\"], 0.4364357804719848, 0.25]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"general\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"george\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.4709733547569204\t[[\"case\", \"government\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.35064935064935066\t[[\"case\", \"in\"], 0.42857142857142855, 0.2727272727272727]\t\r\n",
      "0.4709733547569204\t[[\"case\", \"limited\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"narrative\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.30629112570081773\t[[\"case\", \"of\"], 0.3903600291794133, 0.2222222222222222]\t\r\n",
      "0.14449111825230682\t[[\"case\", \"religious\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.14449111825230682\t[[\"case\", \"sea\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.8035714285714286\t[[\"case\", \"study\"], 0.8571428571428571, 0.75]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"tales\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.22619047619047616\t[[\"case\", \"the\"], 0.2857142857142857, 0.16666666666666666]\t\r\n",
      "0.3000933476157247\t[[\"case\", \"wales\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.675\t[[\"child's\", \"christmas\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"circumstantial\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"city\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.17430339887498947\t[[\"child's\", \"collection\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.2276709006307398\t[[\"child's\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"child's\", \"government\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.4709733547569204\t[[\"child's\", \"in\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"child's\", \"of\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"child's\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"child's\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"child's\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.675\t[[\"child's\", \"wales\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"circumstantial\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"city\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.17430339887498947\t[[\"christmas\", \"collection\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.2276709006307398\t[[\"christmas\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"christmas\", \"government\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.4709733547569204\t[[\"christmas\", \"in\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"christmas\", \"of\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"christmas\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"christmas\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"christmas\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.675\t[[\"christmas\", \"wales\"], 0.75, 0.6]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"city\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.36646394060712184\t[[\"circumstantial\", \"collection\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.19642857142857142\t[[\"circumstantial\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"fairy\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"female\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"circumstantial\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.48867513459481293\t[[\"circumstantial\", \"forms\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"general\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"george\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"circumstantial\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"circumstantial\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"limited\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.675\t[[\"circumstantial\", \"narrative\"], 0.75, 0.6]\t\r\n",
      "0.28739916731037085\t[[\"circumstantial\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"circumstantial\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"sea\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"circumstantial\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.41666666666666663\t[[\"circumstantial\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.4709733547569204\t[[\"circumstantial\", \"the\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"circumstantial\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.17430339887498947\t[[\"city\", \"collection\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"establishing\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.2276709006307398\t[[\"city\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"city\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"city\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.18792297428534527\t[[\"city\", \"of\"], 0.2581988897471611, 0.11764705882352941]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.675\t[[\"city\", \"sea\"], 0.75, 0.6]\t\r\n",
      "0.14449111825230682\t[[\"city\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"city\", \"the\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"city\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"establishing\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.5854101966249685\t[[\"collection\", \"fairy\"], 0.6708203932499369, 0.5]\t\r\n",
      "0.36646394060712184\t[[\"collection\", \"female\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"for\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.4248655564138277\t[[\"collection\", \"forms\"], 0.5163977794943222, 0.3333333333333333]\t\r\n",
      "0.36646394060712184\t[[\"collection\", \"general\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.36646394060712184\t[[\"collection\", \"george\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"government\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.1299699709273971\t[[\"collection\", \"in\"], 0.1690308509457033, 0.09090909090909091]\t\r\n",
      "0.36646394060712184\t[[\"collection\", \"limited\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.36646394060712184\t[[\"collection\", \"narrative\"], 0.4472135954999579, 0.2857142857142857]\t\r\n",
      "0.35594010767585027\t[[\"collection\", \"of\"], 0.46188021535170054, 0.25]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"religious\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"sea\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.26903085094570334\t[[\"collection\", \"study\"], 0.3380617018914066, 0.2]\t\r\n",
      "0.5854101966249685\t[[\"collection\", \"tales\"], 0.6708203932499369, 0.5]\t\r\n",
      "0.26903085094570334\t[[\"collection\", \"the\"], 0.3380617018914066, 0.2]\t\r\n",
      "0.17430339887498947\t[[\"collection\", \"wales\"], 0.22360679774997896, 0.125]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"fairy\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"female\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.675\t[[\"establishing\", \"for\"], 0.75, 0.6]\t\r\n",
      "0.2276709006307398\t[[\"establishing\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"establishing\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"establishing\", \"of\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.675\t[[\"establishing\", \"religious\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"establishing\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"establishing\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"establishing\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"fairy\", \"female\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"fairy\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.8080127018922194\t[[\"fairy\", \"forms\"], 0.8660254037844387, 0.75]\t\r\n",
      "0.41666666666666663\t[[\"fairy\", \"general\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"fairy\", \"george\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.19642857142857142\t[[\"fairy\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"fairy\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"fairy\", \"limited\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"fairy\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"fairy\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"fairy\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"fairy\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"fairy\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.675\t[[\"fairy\", \"tales\"], 0.75, 0.6]\t\r\n",
      "0.3000933476157247\t[[\"fairy\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"fairy\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"female\", \"for\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.48867513459481293\t[[\"female\", \"forms\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.41666666666666663\t[[\"female\", \"general\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"female\", \"george\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.675\t[[\"female\", \"government\"], 0.75, 0.6]\t\r\n",
      "0.4709733547569204\t[[\"female\", \"in\"], 0.5669467095138409, 0.375]\t\r\n",
      "1.0\t[[\"female\", \"limited\"], 1.0, 1.0]\t\r\n",
      "0.41666666666666663\t[[\"female\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"female\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"female\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"female\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"female\", \"study\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.41666666666666663\t[[\"female\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"female\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"female\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.2276709006307398\t[[\"for\", \"forms\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"general\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"george\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"for\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"limited\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"for\", \"of\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.675\t[[\"for\", \"religious\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"for\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"for\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"for\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.48867513459481293\t[[\"forms\", \"general\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.48867513459481293\t[[\"forms\", \"george\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.2276709006307398\t[[\"forms\", \"government\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.16466450067355176\t[[\"forms\", \"in\"], 0.2182178902359924, 0.1111111111111111]\t\r\n",
      "0.48867513459481293\t[[\"forms\", \"limited\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.48867513459481293\t[[\"forms\", \"narrative\"], 0.5773502691896258, 0.4]\t\r\n",
      "0.21157119849998599\t[[\"forms\", \"of\"], 0.29814239699997197, 0.125]\t\r\n",
      "0.2276709006307398\t[[\"forms\", \"religious\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.2276709006307398\t[[\"forms\", \"sea\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.34321789023599236\t[[\"forms\", \"study\"], 0.4364357804719848, 0.25]\t\r\n",
      "0.8080127018922194\t[[\"forms\", \"tales\"], 0.8660254037844387, 0.75]\t\r\n",
      "0.34321789023599236\t[[\"forms\", \"the\"], 0.4364357804719848, 0.25]\t\r\n",
      "0.2276709006307398\t[[\"forms\", \"wales\"], 0.2886751345948129, 0.16666666666666666]\t\r\n",
      "0.675\t[[\"general\", \"george\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"general\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"general\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"general\", \"limited\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"general\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"general\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"general\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"general\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"general\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.41666666666666663\t[[\"general\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"general\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"general\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"george\", \"government\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"george\", \"in\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"george\", \"limited\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.41666666666666663\t[[\"george\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"george\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"george\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"george\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"george\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.41666666666666663\t[[\"george\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"george\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"george\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"government\", \"in\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.675\t[[\"government\", \"limited\"], 0.75, 0.6]\t\r\n",
      "0.19642857142857142\t[[\"government\", \"narrative\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.28739916731037085\t[[\"government\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"government\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"government\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"government\", \"study\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"government\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"government\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.41666666666666663\t[[\"government\", \"wales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.4709733547569204\t[[\"in\", \"limited\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.14449111825230682\t[[\"in\", \"narrative\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.2253323793633326\t[[\"in\", \"of\"], 0.29277002188455997, 0.15789473684210525]\t\r\n",
      "0.14449111825230682\t[[\"in\", \"religious\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.14449111825230682\t[[\"in\", \"sea\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.35064935064935066\t[[\"in\", \"study\"], 0.42857142857142855, 0.2727272727272727]\t\r\n",
      "0.14449111825230682\t[[\"in\", \"tales\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.10989010989010989\t[[\"in\", \"the\"], 0.14285714285714285, 0.07692307692307693]\t\r\n",
      "0.4709733547569204\t[[\"in\", \"wales\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.41666666666666663\t[[\"limited\", \"narrative\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.28739916731037085\t[[\"limited\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"limited\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.19642857142857142\t[[\"limited\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"limited\", \"study\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.41666666666666663\t[[\"limited\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"limited\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"limited\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.28739916731037085\t[[\"narrative\", \"of\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.19642857142857142\t[[\"narrative\", \"religious\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.41666666666666663\t[[\"narrative\", \"sea\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.3000933476157247\t[[\"narrative\", \"study\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.41666666666666663\t[[\"narrative\", \"tales\"], 0.5, 0.3333333333333333]\t\r\n",
      "0.4709733547569204\t[[\"narrative\", \"the\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"narrative\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.09232750021456805\t[[\"of\", \"religious\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.18792297428534527\t[[\"of\", \"sea\"], 0.2581988897471611, 0.11764705882352941]\t\r\n",
      "0.30629112570081773\t[[\"of\", \"study\"], 0.3903600291794133, 0.2222222222222222]\t\r\n",
      "0.28739916731037085\t[[\"of\", \"tales\"], 0.3872983346207417, 0.1875]\t\r\n",
      "0.2253323793633326\t[[\"of\", \"the\"], 0.29277002188455997, 0.15789473684210525]\t\r\n",
      "0.09232750021456805\t[[\"of\", \"wales\"], 0.12909944487358055, 0.05555555555555555]\t\r\n",
      "0.19642857142857142\t[[\"religious\", \"sea\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"religious\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"religious\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"religious\", \"the\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"religious\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"sea\", \"study\"], 0.1889822365046136, 0.1]\t\r\n",
      "0.19642857142857142\t[[\"sea\", \"tales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.4709733547569204\t[[\"sea\", \"the\"], 0.5669467095138409, 0.375]\t\r\n",
      "0.19642857142857142\t[[\"sea\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.3000933476157247\t[[\"study\", \"tales\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.22619047619047616\t[[\"study\", \"the\"], 0.2857142857142857, 0.16666666666666666]\t\r\n",
      "0.3000933476157247\t[[\"study\", \"wales\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.3000933476157247\t[[\"tales\", \"the\"], 0.3779644730092272, 0.2222222222222222]\t\r\n",
      "0.19642857142857142\t[[\"tales\", \"wales\"], 0.25, 0.14285714285714285]\t\r\n",
      "0.14449111825230682\t[[\"the\", \"wales\"], 0.1889822365046136, 0.1]\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > systems_test_similarities_1\n",
    "!cat systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Similarity measures\n",
      "\n",
      "        average |           pair |         cosine |        jaccard\n",
      "--------------------------------------------------------------------------------\n",
      "       0.197909 |       a - bill |       0.288675 |       0.107143\n",
      "       0.197909 |  a - biography |       0.288675 |       0.107143\n",
      "       0.197909 |         a - by |       0.288675 |       0.107143\n",
      "       0.325361 |       a - case |       0.436436 |       0.214286\n",
      "       0.197909 |    a - child's |       0.288675 |       0.107143\n",
      "       0.197909 |  a - christmas |       0.288675 |       0.107143\n",
      "       0.197909 |a - circumstantial |       0.288675 |       0.107143\n",
      "       0.197909 |       a - city |       0.288675 |       0.107143\n",
      "       0.243561 | a - collection |       0.344265 |       0.142857\n",
      "       0.197909 |a - establishing |       0.288675 |       0.107143\n",
      "       0.197909 |      a - fairy |       0.288675 |       0.107143\n",
      "       0.197909 |     a - female |       0.288675 |       0.107143\n",
      "       0.197909 |        a - for |       0.288675 |       0.107143\n",
      "       0.146825 |      a - forms |       0.222222 |       0.071429\n",
      "       0.197909 |    a - general |       0.288675 |       0.107143\n",
      "       0.197909 |     a - george |       0.288675 |       0.107143\n",
      "       0.197909 | a - government |       0.288675 |       0.107143\n",
      "       0.325361 |         a - in |       0.436436 |       0.214286\n",
      "       0.197909 |    a - limited |       0.288675 |       0.107143\n",
      "       0.197909 |  a - narrative |       0.288675 |       0.107143\n",
      "       0.597833 |         a - of |       0.695666 |       0.500000\n",
      "       0.197909 |  a - religious |       0.288675 |       0.107143\n",
      "       0.197909 |        a - sea |       0.288675 |       0.107143\n",
      "       0.325361 |      a - study |       0.436436 |       0.214286\n",
      "       0.197909 |      a - tales |       0.288675 |       0.107143\n",
      "       0.325361 |        a - the |       0.436436 |       0.214286\n",
      "       0.197909 |      a - wales |       0.288675 |       0.107143\n",
      "       0.196429 |bill - biography |       0.250000 |       0.142857\n",
      "       0.196429 |      bill - by |       0.250000 |       0.142857\n",
      "       0.144491 |    bill - case |       0.188982 |       0.100000\n",
      "       0.196429 | bill - child's |       0.250000 |       0.142857\n",
      "       0.196429 |bill - christmas |       0.250000 |       0.142857\n",
      "       0.196429 |bill - circumstantial |       0.250000 |       0.142857\n",
      "       0.196429 |    bill - city |       0.250000 |       0.142857\n",
      "       0.174303 |bill - collection |       0.223607 |       0.125000\n",
      "       0.675000 |bill - establishing |       0.750000 |       0.600000\n",
      "       0.196429 |   bill - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |  bill - female |       0.250000 |       0.142857\n",
      "       0.675000 |     bill - for |       0.750000 |       0.600000\n",
      "       0.227671 |   bill - forms |       0.288675 |       0.166667\n",
      "       0.196429 | bill - general |       0.250000 |       0.142857\n",
      "       0.196429 |  bill - george |       0.250000 |       0.142857\n",
      "       0.196429 |bill - government |       0.250000 |       0.142857\n",
      "       0.144491 |      bill - in |       0.188982 |       0.100000\n",
      "       0.196429 | bill - limited |       0.250000 |       0.142857\n",
      "       0.196429 |bill - narrative |       0.250000 |       0.142857\n",
      "       0.092328 |      bill - of |       0.129099 |       0.055556\n",
      "       0.675000 |bill - religious |       0.750000 |       0.600000\n",
      "       0.196429 |     bill - sea |       0.250000 |       0.142857\n",
      "       0.144491 |   bill - study |       0.188982 |       0.100000\n",
      "       0.196429 |   bill - tales |       0.250000 |       0.142857\n",
      "       0.144491 |     bill - the |       0.188982 |       0.100000\n",
      "       0.196429 |   bill - wales |       0.250000 |       0.142857\n",
      "       0.196429 | biography - by |       0.250000 |       0.142857\n",
      "       0.300093 |biography - case |       0.377964 |       0.222222\n",
      "       0.196429 |biography - child's |       0.250000 |       0.142857\n",
      "       0.196429 |biography - christmas |       0.250000 |       0.142857\n",
      "       0.416667 |biography - circumstantial |       0.500000 |       0.333333\n",
      "       0.196429 |biography - city |       0.250000 |       0.142857\n",
      "       0.366464 |biography - collection |       0.447214 |       0.285714\n",
      "       0.196429 |biography - establishing |       0.250000 |       0.142857\n",
      "       0.416667 |biography - fairy |       0.500000 |       0.333333\n",
      "       0.416667 |biography - female |       0.500000 |       0.333333\n",
      "       0.196429 |biography - for |       0.250000 |       0.142857\n",
      "       0.488675 |biography - forms |       0.577350 |       0.400000\n",
      "       0.675000 |biography - general |       0.750000 |       0.600000\n",
      "       0.675000 |biography - george |       0.750000 |       0.600000\n",
      "       0.196429 |biography - government |       0.250000 |       0.142857\n",
      "       0.144491 | biography - in |       0.188982 |       0.100000\n",
      "       0.416667 |biography - limited |       0.500000 |       0.333333\n",
      "       0.416667 |biography - narrative |       0.500000 |       0.333333\n",
      "       0.287399 | biography - of |       0.387298 |       0.187500\n",
      "       0.196429 |biography - religious |       0.250000 |       0.142857\n",
      "       0.196429 |biography - sea |       0.250000 |       0.142857\n",
      "       0.300093 |biography - study |       0.377964 |       0.222222\n",
      "       0.416667 |biography - tales |       0.500000 |       0.333333\n",
      "       0.300093 |biography - the |       0.377964 |       0.222222\n",
      "       0.196429 |biography - wales |       0.250000 |       0.142857\n",
      "       0.144491 |      by - case |       0.188982 |       0.100000\n",
      "       0.196429 |   by - child's |       0.250000 |       0.142857\n",
      "       0.196429 | by - christmas |       0.250000 |       0.142857\n",
      "       0.416667 |by - circumstantial |       0.500000 |       0.333333\n",
      "       0.675000 |      by - city |       0.750000 |       0.600000\n",
      "       0.174303 |by - collection |       0.223607 |       0.125000\n",
      "       0.196429 |by - establishing |       0.250000 |       0.142857\n",
      "       0.196429 |     by - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |    by - female |       0.250000 |       0.142857\n",
      "       0.196429 |       by - for |       0.250000 |       0.142857\n",
      "       0.227671 |     by - forms |       0.288675 |       0.166667\n",
      "       0.196429 |   by - general |       0.250000 |       0.142857\n",
      "       0.196429 |    by - george |       0.250000 |       0.142857\n",
      "       0.196429 |by - government |       0.250000 |       0.142857\n",
      "       0.144491 |        by - in |       0.188982 |       0.100000\n",
      "       0.196429 |   by - limited |       0.250000 |       0.142857\n",
      "       0.416667 | by - narrative |       0.500000 |       0.333333\n",
      "       0.187923 |        by - of |       0.258199 |       0.117647\n",
      "       0.196429 | by - religious |       0.250000 |       0.142857\n",
      "       0.675000 |       by - sea |       0.750000 |       0.600000\n",
      "       0.144491 |     by - study |       0.188982 |       0.100000\n",
      "       0.196429 |     by - tales |       0.250000 |       0.142857\n",
      "       0.470973 |       by - the |       0.566947 |       0.375000\n",
      "       0.196429 |     by - wales |       0.250000 |       0.142857\n",
      "       0.300093 | case - child's |       0.377964 |       0.222222\n",
      "       0.300093 |case - christmas |       0.377964 |       0.222222\n",
      "       0.300093 |case - circumstantial |       0.377964 |       0.222222\n",
      "       0.144491 |    case - city |       0.188982 |       0.100000\n",
      "       0.269031 |case - collection |       0.338062 |       0.200000\n",
      "       0.144491 |case - establishing |       0.188982 |       0.100000\n",
      "       0.300093 |   case - fairy |       0.377964 |       0.222222\n",
      "       0.470973 |  case - female |       0.566947 |       0.375000\n",
      "       0.144491 |     case - for |       0.188982 |       0.100000\n",
      "       0.343218 |   case - forms |       0.436436 |       0.250000\n",
      "       0.300093 | case - general |       0.377964 |       0.222222\n",
      "       0.300093 |  case - george |       0.377964 |       0.222222\n",
      "       0.470973 |case - government |       0.566947 |       0.375000\n",
      "       0.350649 |      case - in |       0.428571 |       0.272727\n",
      "       0.470973 | case - limited |       0.566947 |       0.375000\n",
      "       0.300093 |case - narrative |       0.377964 |       0.222222\n",
      "       0.306291 |      case - of |       0.390360 |       0.222222\n",
      "       0.144491 |case - religious |       0.188982 |       0.100000\n",
      "       0.144491 |     case - sea |       0.188982 |       0.100000\n",
      "       0.803571 |   case - study |       0.857143 |       0.750000\n",
      "       0.300093 |   case - tales |       0.377964 |       0.222222\n",
      "       0.226190 |     case - the |       0.285714 |       0.166667\n",
      "       0.300093 |   case - wales |       0.377964 |       0.222222\n",
      "       0.675000 |child's - christmas |       0.750000 |       0.600000\n",
      "       0.196429 |child's - circumstantial |       0.250000 |       0.142857\n",
      "       0.196429 | child's - city |       0.250000 |       0.142857\n",
      "       0.174303 |child's - collection |       0.223607 |       0.125000\n",
      "       0.196429 |child's - establishing |       0.250000 |       0.142857\n",
      "       0.196429 |child's - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |child's - female |       0.250000 |       0.142857\n",
      "       0.196429 |  child's - for |       0.250000 |       0.142857\n",
      "       0.227671 |child's - forms |       0.288675 |       0.166667\n",
      "       0.196429 |child's - general |       0.250000 |       0.142857\n",
      "       0.196429 |child's - george |       0.250000 |       0.142857\n",
      "       0.416667 |child's - government |       0.500000 |       0.333333\n",
      "       0.470973 |   child's - in |       0.566947 |       0.375000\n",
      "       0.196429 |child's - limited |       0.250000 |       0.142857\n",
      "       0.196429 |child's - narrative |       0.250000 |       0.142857\n",
      "       0.092328 |   child's - of |       0.129099 |       0.055556\n",
      "       0.196429 |child's - religious |       0.250000 |       0.142857\n",
      "       0.196429 |  child's - sea |       0.250000 |       0.142857\n",
      "       0.300093 |child's - study |       0.377964 |       0.222222\n",
      "       0.196429 |child's - tales |       0.250000 |       0.142857\n",
      "       0.144491 |  child's - the |       0.188982 |       0.100000\n",
      "       0.675000 |child's - wales |       0.750000 |       0.600000\n",
      "       0.196429 |christmas - circumstantial |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - city |       0.250000 |       0.142857\n",
      "       0.174303 |christmas - collection |       0.223607 |       0.125000\n",
      "       0.196429 |christmas - establishing |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - female |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - for |       0.250000 |       0.142857\n",
      "       0.227671 |christmas - forms |       0.288675 |       0.166667\n",
      "       0.196429 |christmas - general |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - george |       0.250000 |       0.142857\n",
      "       0.416667 |christmas - government |       0.500000 |       0.333333\n",
      "       0.470973 | christmas - in |       0.566947 |       0.375000\n",
      "       0.196429 |christmas - limited |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - narrative |       0.250000 |       0.142857\n",
      "       0.092328 | christmas - of |       0.129099 |       0.055556\n",
      "       0.196429 |christmas - religious |       0.250000 |       0.142857\n",
      "       0.196429 |christmas - sea |       0.250000 |       0.142857\n",
      "       0.300093 |christmas - study |       0.377964 |       0.222222\n",
      "       0.196429 |christmas - tales |       0.250000 |       0.142857\n",
      "       0.144491 |christmas - the |       0.188982 |       0.100000\n",
      "       0.675000 |christmas - wales |       0.750000 |       0.600000\n",
      "       0.416667 |circumstantial - city |       0.500000 |       0.333333\n",
      "       0.366464 |circumstantial - collection |       0.447214 |       0.285714\n",
      "       0.196429 |circumstantial - establishing |       0.250000 |       0.142857\n",
      "       0.416667 |circumstantial - fairy |       0.500000 |       0.333333\n",
      "       0.416667 |circumstantial - female |       0.500000 |       0.333333\n",
      "       0.196429 |circumstantial - for |       0.250000 |       0.142857\n",
      "       0.488675 |circumstantial - forms |       0.577350 |       0.400000\n",
      "       0.416667 |circumstantial - general |       0.500000 |       0.333333\n",
      "       0.416667 |circumstantial - george |       0.500000 |       0.333333\n",
      "       0.196429 |circumstantial - government |       0.250000 |       0.142857\n",
      "       0.144491 |circumstantial - in |       0.188982 |       0.100000\n",
      "       0.416667 |circumstantial - limited |       0.500000 |       0.333333\n",
      "       0.675000 |circumstantial - narrative |       0.750000 |       0.600000\n",
      "       0.287399 |circumstantial - of |       0.387298 |       0.187500\n",
      "       0.196429 |circumstantial - religious |       0.250000 |       0.142857\n",
      "       0.416667 |circumstantial - sea |       0.500000 |       0.333333\n",
      "       0.300093 |circumstantial - study |       0.377964 |       0.222222\n",
      "       0.416667 |circumstantial - tales |       0.500000 |       0.333333\n",
      "       0.470973 |circumstantial - the |       0.566947 |       0.375000\n",
      "       0.196429 |circumstantial - wales |       0.250000 |       0.142857\n",
      "       0.174303 |city - collection |       0.223607 |       0.125000\n",
      "       0.196429 |city - establishing |       0.250000 |       0.142857\n",
      "       0.196429 |   city - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |  city - female |       0.250000 |       0.142857\n",
      "       0.196429 |     city - for |       0.250000 |       0.142857\n",
      "       0.227671 |   city - forms |       0.288675 |       0.166667\n",
      "       0.196429 | city - general |       0.250000 |       0.142857\n",
      "       0.196429 |  city - george |       0.250000 |       0.142857\n",
      "       0.196429 |city - government |       0.250000 |       0.142857\n",
      "       0.144491 |      city - in |       0.188982 |       0.100000\n",
      "       0.196429 | city - limited |       0.250000 |       0.142857\n",
      "       0.416667 |city - narrative |       0.500000 |       0.333333\n",
      "       0.187923 |      city - of |       0.258199 |       0.117647\n",
      "       0.196429 |city - religious |       0.250000 |       0.142857\n",
      "       0.675000 |     city - sea |       0.750000 |       0.600000\n",
      "       0.144491 |   city - study |       0.188982 |       0.100000\n",
      "       0.196429 |   city - tales |       0.250000 |       0.142857\n",
      "       0.470973 |     city - the |       0.566947 |       0.375000\n",
      "       0.196429 |   city - wales |       0.250000 |       0.142857\n",
      "       0.174303 |collection - establishing |       0.223607 |       0.125000\n",
      "       0.585410 |collection - fairy |       0.670820 |       0.500000\n",
      "       0.366464 |collection - female |       0.447214 |       0.285714\n",
      "       0.174303 |collection - for |       0.223607 |       0.125000\n",
      "       0.424866 |collection - forms |       0.516398 |       0.333333\n",
      "       0.366464 |collection - general |       0.447214 |       0.285714\n",
      "       0.366464 |collection - george |       0.447214 |       0.285714\n",
      "       0.174303 |collection - government |       0.223607 |       0.125000\n",
      "       0.129970 |collection - in |       0.169031 |       0.090909\n",
      "       0.366464 |collection - limited |       0.447214 |       0.285714\n",
      "       0.366464 |collection - narrative |       0.447214 |       0.285714\n",
      "       0.355940 |collection - of |       0.461880 |       0.250000\n",
      "       0.174303 |collection - religious |       0.223607 |       0.125000\n",
      "       0.174303 |collection - sea |       0.223607 |       0.125000\n",
      "       0.269031 |collection - study |       0.338062 |       0.200000\n",
      "       0.585410 |collection - tales |       0.670820 |       0.500000\n",
      "       0.269031 |collection - the |       0.338062 |       0.200000\n",
      "       0.174303 |collection - wales |       0.223607 |       0.125000\n",
      "       0.196429 |establishing - fairy |       0.250000 |       0.142857\n",
      "       0.196429 |establishing - female |       0.250000 |       0.142857\n",
      "       0.675000 |establishing - for |       0.750000 |       0.600000\n",
      "       0.227671 |establishing - forms |       0.288675 |       0.166667\n",
      "       0.196429 |establishing - general |       0.250000 |       0.142857\n",
      "       0.196429 |establishing - george |       0.250000 |       0.142857\n",
      "       0.196429 |establishing - government |       0.250000 |       0.142857\n",
      "       0.144491 |establishing - in |       0.188982 |       0.100000\n",
      "       0.196429 |establishing - limited |       0.250000 |       0.142857\n",
      "       0.196429 |establishing - narrative |       0.250000 |       0.142857\n",
      "       0.092328 |establishing - of |       0.129099 |       0.055556\n",
      "       0.675000 |establishing - religious |       0.750000 |       0.600000\n",
      "       0.196429 |establishing - sea |       0.250000 |       0.142857\n",
      "       0.144491 |establishing - study |       0.188982 |       0.100000\n",
      "       0.196429 |establishing - tales |       0.250000 |       0.142857\n",
      "       0.144491 |establishing - the |       0.188982 |       0.100000\n",
      "       0.196429 |establishing - wales |       0.250000 |       0.142857\n",
      "       0.416667 | fairy - female |       0.500000 |       0.333333\n",
      "       0.196429 |    fairy - for |       0.250000 |       0.142857\n",
      "       0.808013 |  fairy - forms |       0.866025 |       0.750000\n",
      "       0.416667 |fairy - general |       0.500000 |       0.333333\n",
      "       0.416667 | fairy - george |       0.500000 |       0.333333\n",
      "       0.196429 |fairy - government |       0.250000 |       0.142857\n",
      "       0.144491 |     fairy - in |       0.188982 |       0.100000\n",
      "       0.416667 |fairy - limited |       0.500000 |       0.333333\n",
      "       0.416667 |fairy - narrative |       0.500000 |       0.333333\n",
      "       0.287399 |     fairy - of |       0.387298 |       0.187500\n",
      "       0.196429 |fairy - religious |       0.250000 |       0.142857\n",
      "       0.196429 |    fairy - sea |       0.250000 |       0.142857\n",
      "       0.300093 |  fairy - study |       0.377964 |       0.222222\n",
      "       0.675000 |  fairy - tales |       0.750000 |       0.600000\n",
      "       0.300093 |    fairy - the |       0.377964 |       0.222222\n",
      "       0.196429 |  fairy - wales |       0.250000 |       0.142857\n",
      "       0.196429 |   female - for |       0.250000 |       0.142857\n",
      "       0.488675 | female - forms |       0.577350 |       0.400000\n",
      "       0.416667 |female - general |       0.500000 |       0.333333\n",
      "       0.416667 |female - george |       0.500000 |       0.333333\n",
      "       0.675000 |female - government |       0.750000 |       0.600000\n",
      "       0.470973 |    female - in |       0.566947 |       0.375000\n",
      "       1.000000 |female - limited |       1.000000 |       1.000000\n",
      "       0.416667 |female - narrative |       0.500000 |       0.333333\n",
      "       0.287399 |    female - of |       0.387298 |       0.187500\n",
      "       0.196429 |female - religious |       0.250000 |       0.142857\n",
      "       0.196429 |   female - sea |       0.250000 |       0.142857\n",
      "       0.470973 | female - study |       0.566947 |       0.375000\n",
      "       0.416667 | female - tales |       0.500000 |       0.333333\n",
      "       0.300093 |   female - the |       0.377964 |       0.222222\n",
      "       0.196429 | female - wales |       0.250000 |       0.142857\n",
      "       0.227671 |    for - forms |       0.288675 |       0.166667\n",
      "       0.196429 |  for - general |       0.250000 |       0.142857\n",
      "       0.196429 |   for - george |       0.250000 |       0.142857\n",
      "       0.196429 |for - government |       0.250000 |       0.142857\n",
      "       0.144491 |       for - in |       0.188982 |       0.100000\n",
      "       0.196429 |  for - limited |       0.250000 |       0.142857\n",
      "       0.196429 |for - narrative |       0.250000 |       0.142857\n",
      "       0.092328 |       for - of |       0.129099 |       0.055556\n",
      "       0.675000 |for - religious |       0.750000 |       0.600000\n",
      "       0.196429 |      for - sea |       0.250000 |       0.142857\n",
      "       0.144491 |    for - study |       0.188982 |       0.100000\n",
      "       0.196429 |    for - tales |       0.250000 |       0.142857\n",
      "       0.144491 |      for - the |       0.188982 |       0.100000\n",
      "       0.196429 |    for - wales |       0.250000 |       0.142857\n",
      "       0.488675 |forms - general |       0.577350 |       0.400000\n",
      "       0.488675 | forms - george |       0.577350 |       0.400000\n",
      "       0.227671 |forms - government |       0.288675 |       0.166667\n",
      "       0.164665 |     forms - in |       0.218218 |       0.111111\n",
      "       0.488675 |forms - limited |       0.577350 |       0.400000\n",
      "       0.488675 |forms - narrative |       0.577350 |       0.400000\n",
      "       0.211571 |     forms - of |       0.298142 |       0.125000\n",
      "       0.227671 |forms - religious |       0.288675 |       0.166667\n",
      "       0.227671 |    forms - sea |       0.288675 |       0.166667\n",
      "       0.343218 |  forms - study |       0.436436 |       0.250000\n",
      "       0.808013 |  forms - tales |       0.866025 |       0.750000\n",
      "       0.343218 |    forms - the |       0.436436 |       0.250000\n",
      "       0.227671 |  forms - wales |       0.288675 |       0.166667\n",
      "       0.675000 |general - george |       0.750000 |       0.600000\n",
      "       0.196429 |general - government |       0.250000 |       0.142857\n",
      "       0.144491 |   general - in |       0.188982 |       0.100000\n",
      "       0.416667 |general - limited |       0.500000 |       0.333333\n",
      "       0.416667 |general - narrative |       0.500000 |       0.333333\n",
      "       0.287399 |   general - of |       0.387298 |       0.187500\n",
      "       0.196429 |general - religious |       0.250000 |       0.142857\n",
      "       0.196429 |  general - sea |       0.250000 |       0.142857\n",
      "       0.300093 |general - study |       0.377964 |       0.222222\n",
      "       0.416667 |general - tales |       0.500000 |       0.333333\n",
      "       0.300093 |  general - the |       0.377964 |       0.222222\n",
      "       0.196429 |general - wales |       0.250000 |       0.142857\n",
      "       0.196429 |george - government |       0.250000 |       0.142857\n",
      "       0.144491 |    george - in |       0.188982 |       0.100000\n",
      "       0.416667 |george - limited |       0.500000 |       0.333333\n",
      "       0.416667 |george - narrative |       0.500000 |       0.333333\n",
      "       0.287399 |    george - of |       0.387298 |       0.187500\n",
      "       0.196429 |george - religious |       0.250000 |       0.142857\n",
      "       0.196429 |   george - sea |       0.250000 |       0.142857\n",
      "       0.300093 | george - study |       0.377964 |       0.222222\n",
      "       0.416667 | george - tales |       0.500000 |       0.333333\n",
      "       0.300093 |   george - the |       0.377964 |       0.222222\n",
      "       0.196429 | george - wales |       0.250000 |       0.142857\n",
      "       0.470973 |government - in |       0.566947 |       0.375000\n",
      "       0.675000 |government - limited |       0.750000 |       0.600000\n",
      "       0.196429 |government - narrative |       0.250000 |       0.142857\n",
      "       0.287399 |government - of |       0.387298 |       0.187500\n",
      "       0.196429 |government - religious |       0.250000 |       0.142857\n",
      "       0.196429 |government - sea |       0.250000 |       0.142857\n",
      "       0.470973 |government - study |       0.566947 |       0.375000\n",
      "       0.196429 |government - tales |       0.250000 |       0.142857\n",
      "       0.144491 |government - the |       0.188982 |       0.100000\n",
      "       0.416667 |government - wales |       0.500000 |       0.333333\n",
      "       0.470973 |   in - limited |       0.566947 |       0.375000\n",
      "       0.144491 | in - narrative |       0.188982 |       0.100000\n",
      "       0.225332 |        in - of |       0.292770 |       0.157895\n",
      "       0.144491 | in - religious |       0.188982 |       0.100000\n",
      "       0.144491 |       in - sea |       0.188982 |       0.100000\n",
      "       0.350649 |     in - study |       0.428571 |       0.272727\n",
      "       0.144491 |     in - tales |       0.188982 |       0.100000\n",
      "       0.109890 |       in - the |       0.142857 |       0.076923\n",
      "       0.470973 |     in - wales |       0.566947 |       0.375000\n",
      "       0.416667 |limited - narrative |       0.500000 |       0.333333\n",
      "       0.287399 |   limited - of |       0.387298 |       0.187500\n",
      "       0.196429 |limited - religious |       0.250000 |       0.142857\n",
      "       0.196429 |  limited - sea |       0.250000 |       0.142857\n",
      "       0.470973 |limited - study |       0.566947 |       0.375000\n",
      "       0.416667 |limited - tales |       0.500000 |       0.333333\n",
      "       0.300093 |  limited - the |       0.377964 |       0.222222\n",
      "       0.196429 |limited - wales |       0.250000 |       0.142857\n",
      "       0.287399 | narrative - of |       0.387298 |       0.187500\n",
      "       0.196429 |narrative - religious |       0.250000 |       0.142857\n",
      "       0.416667 |narrative - sea |       0.500000 |       0.333333\n",
      "       0.300093 |narrative - study |       0.377964 |       0.222222\n",
      "       0.416667 |narrative - tales |       0.500000 |       0.333333\n",
      "       0.470973 |narrative - the |       0.566947 |       0.375000\n",
      "       0.196429 |narrative - wales |       0.250000 |       0.142857\n",
      "       0.092328 | of - religious |       0.129099 |       0.055556\n",
      "       0.187923 |       of - sea |       0.258199 |       0.117647\n",
      "       0.306291 |     of - study |       0.390360 |       0.222222\n",
      "       0.287399 |     of - tales |       0.387298 |       0.187500\n",
      "       0.225332 |       of - the |       0.292770 |       0.157895\n",
      "       0.092328 |     of - wales |       0.129099 |       0.055556\n",
      "       0.196429 |religious - sea |       0.250000 |       0.142857\n",
      "       0.144491 |religious - study |       0.188982 |       0.100000\n",
      "       0.196429 |religious - tales |       0.250000 |       0.142857\n",
      "       0.144491 |religious - the |       0.188982 |       0.100000\n",
      "       0.196429 |religious - wales |       0.250000 |       0.142857\n",
      "       0.144491 |    sea - study |       0.188982 |       0.100000\n",
      "       0.196429 |    sea - tales |       0.250000 |       0.142857\n",
      "       0.470973 |      sea - the |       0.566947 |       0.375000\n",
      "       0.196429 |    sea - wales |       0.250000 |       0.142857\n",
      "       0.300093 |  study - tales |       0.377964 |       0.222222\n",
      "       0.226190 |    study - the |       0.285714 |       0.166667\n",
      "       0.300093 |  study - wales |       0.377964 |       0.222222\n",
      "       0.300093 |    tales - the |       0.377964 |       0.222222\n",
      "       0.196429 |  tales - wales |       0.250000 |       0.142857\n",
      "       0.144491 |    the - wales |       0.188982 |       0.100000\n",
      "\n",
      "Systems test  2  - Similarity measures\n",
      "\n",
      "        average |           pair |         cosine |        jaccard\n",
      "--------------------------------------------------------------------------------\n",
      "       0.583333 |  boon - dipped |       0.666667 |       0.500000\n",
      "       0.329124 |  cava - dipped |       0.408248 |       0.250000\n",
      "       0.329124 |   atlas - boon |       0.408248 |       0.250000\n",
      "       1.000000 |   atlas - cava |       1.000000 |       1.000000\n",
      "       0.329124 | atlas - dipped |       0.408248 |       0.250000\n",
      "       0.329124 |    boon - cava |       0.408248 |       0.250000\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,3):\n",
    "  print ''*80\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print ''*80\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\")\n",
    "  print '-'*80\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            word1 = stripe[0][0]\n",
    "            word2 = stripe[0][1]\n",
    "            #for word in words:\n",
    "            #    print(word)\n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f}\".format(\n",
    "                float(avg), word1 + \" - \" + word2, float(stripe[1]), float(stripe[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.6 -Google n-grams EDA\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\r\n",
      "A Biography of General George\t92\t90\t74\r\n",
      "A Case Study in Government\t102\t102\t78\r\n",
      "A Case Study of Female\t447\t447\t327\r\n",
      "A Case Study of Limited\t55\t55\t43\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.A\n",
    "    def mapper1(self, _, line):\n",
    "        ngram, count, pages_count, books_count =  line.strip().lower().split(\"\\t\")\n",
    "        yield len(ngram), ngram\n",
    "   \n",
    "    def reducer1(self, key, values):\n",
    "        for value in values:\n",
    "            yield key, value\n",
    "        \n",
    "        \n",
    "    #JOBCONF = {\n",
    "    #    'mapreduce.job.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "    #    'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "    #    'mapreduce.job.reduces': '1'}\n",
    "\n",
    "  \n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep( #jobconf = self.JOBCONF,\n",
    "                        mapper=self.mapper1,\n",
    "                        reducer=self.reducer1)]    \n",
    " \n",
    "    # END STUDENT CODE 5.6.1.A\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    longest5gram.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/longest5gram.daghan.20180307.184409.540724\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/longest5gram.daghan.20180307.184409.540724/output...\n",
      "Removing temp directory /tmp/longest5gram.daghan.20180307.184409.540724...\n",
      "26\t\"a case study in government\"\n",
      "22\t\"a case study of female\"\n",
      "27\t\"a collection of fairy tales\"\n",
      "23\t\"a case study of limited\"\n",
      "28\t\"a child's christmas in wales\"\n",
      "24\t\"a collection of forms of\"\n",
      "17\t\"a city by the sea\"\n",
      "29\t\"a biography of general george\"\n",
      "33\t\"a bill for establishing religious\"\n",
      "33\t\"a circumstantial narrative of the\"\n"
     ]
    }
   ],
   "source": [
    "!python longest5gram.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > tmp.txt\n",
    "!cat tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 18:32:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/longest5gram.daghan.20180307.183233.170812\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/longest5gram.daghan.20180307.183233.170812/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6699303559114422145.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_28852\n",
      "  Submitted application application_1509050304403_28852\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_28852/\n",
      "  Running job: job_1509050304403_28852\n",
      "  Job job_1509050304403_28852 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_28852 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_1\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9885119\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5241286\n",
      "\t\tFILE: Number of bytes written=11277764\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489801\n",
      "\t\tHDFS: Number of bytes written=9885119\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=28773888\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=71964160\n",
      "\t\tTotal time spent by all map tasks (ms)=18733\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56199\n",
      "\t\tTotal time spent by all reduce tasks (ms)=28111\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=140555\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18733\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=28111\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=20080\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=664\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=10196733\n",
      "\t\tMap output materialized bytes=5243828\n",
      "\t\tMap output records=311614\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tPhysical memory (bytes) snapshot=2930106368\n",
      "\t\tReduce input groups=311614\n",
      "\t\tReduce input records=311614\n",
      "\t\tReduce output records=311614\n",
      "\t\tReduce shuffle bytes=5243828\n",
      "\t\tShuffled Maps =8\n",
      "\t\tSpilled Records=623228\n",
      "\t\tTotal committed heap usage (bytes)=3719299072\n",
      "\t\tVirtual memory (bytes) snapshot=25145917440\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/longest5gram.daghan.20180307.183233.170812...\n",
      "Removing temp directory /tmp/longest5gram.daghan.20180307.183233.170812...\n",
      "55\t\"oligonucleotide arrays using semiconductor photoresists\"\n",
      "55\t\"differential reinforcement of successive approximations\"\n",
      "55\t\"diabetic glomerulopathy by pharmacological amelioration\"\n",
      "55\t\"prevention of experimental autoimmune encephalomyelitis\"\n",
      "51\t\"fractional distillation and subsequent purification\"\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python longest5gram.py -r hadoop hdfs://{TEST_1} \\\n",
    "    --jobconf mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    --jobconf mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "    --jobconf mapreduce.job.reduces=4 \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_1.txt\n",
    "!head -n 5 test_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\t\"oligonucleotide arrays using semiconductor photoresists\"\n",
      "55\t\"differential reinforcement of successive approximations\"\n",
      "55\t\"diabetic glomerulopathy by pharmacological amelioration\"\n",
      "55\t\"prevention of experimental autoimmune encephalomyelitis\"\n",
      "51\t\"fractional distillation and subsequent purification\"\n",
      "cat: Unable to write to output stream.\n",
      "52\t\"international organizations or between international\"\n",
      "52\t\"information processing and communications technology\"\n",
      "52\t\"competitive orientation on intergroup discrimination\"\n",
      "52\t\"ultrastructural classification of pituitary adenomas\"\n",
      "52\t\"professional preparation of counseling psychologists\"\n",
      "cat: Unable to write to output stream.\n",
      "57\t\"der verfassungsgebenden deutschen nationalversammlung und\"\n",
      "57\t\"guidelines for clinical intracardiac electrophysiological\"\n",
      "53\t\"mechanical planarization of microelectronic materials\"\n",
      "53\t\"historical institutionalism in contemporary political\"\n",
      "53\t\"professional technical reference tomorrow's solutions\"\n",
      "cat: Unable to write to output stream.\n",
      "58\t\"interpersonal communication interpersonal communication is\"\n",
      "58\t\"hydroxytryptamine stimulates inositol phosphate production\"\n",
      "54\t\"pulmonary dysfunction following traumatic quadriplegia\"\n",
      "54\t\"east hertfordshire archaeological society transactions\"\n",
      "50\t\"exceptionally meritorious and conspicuous services\"\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -ls  {OUTPUT_PATH}\n",
    "for idx in range(4):\n",
    "    !hdfs dfs -cat {OUTPUT_PATH}/part-0000{idx} | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 06:18:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_20' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/longest5gram.daghan.20180307.061849.170956\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/longest5gram.daghan.20180307.061849.170956/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3522958349325494080.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1509050304403_27328\n",
      "  Submitted application application_1509050304403_27328\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27328/\n",
      "  Running job: job_1509050304403_27328\n",
      "  Job job_1509050304403_27328 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 59%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_27328 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_20\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=217918535\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=188223496\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=82061724\n",
      "\t\tFILE: Number of bytes written=180917655\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217921825\n",
      "\t\tHDFS: Number of bytes written=188223496\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=63\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=21\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=20\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=618642432\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=471098880\n",
      "\t\tTotal time spent by all map tasks (ms)=402762\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1208286\n",
      "\t\tTotal time spent by all reduce tasks (ms)=184023\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=920115\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=402762\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=184023\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=401140\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=7150\n",
      "\t\tInput split bytes=3290\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=194154803\n",
      "\t\tMap output materialized bytes=96077529\n",
      "\t\tMap output records=5931307\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tPhysical memory (bytes) snapshot=16994783232\n",
      "\t\tReduce input groups=5922640\n",
      "\t\tReduce input records=5931307\n",
      "\t\tReduce output records=5931307\n",
      "\t\tReduce shuffle bytes=96077529\n",
      "\t\tShuffled Maps =20\n",
      "\t\tSpilled Records=11862614\n",
      "\t\tTotal committed heap usage (bytes)=18774753280\n",
      "\t\tVirtual memory (bytes) snapshot=73008828416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/longest5gram.daghan.20180307.061849.170956...\n",
      "Removing temp directory /tmp/longest5gram.daghan.20180307.061849.170956...\n",
      "83\t\"engineeringgraphic design applied sciencespsychology interiordesignbiologychemistry\"\n",
      "79\t\"tyxqaswedbrushe zxcvframeqasfuc beretxcvwaterze colorsxzqwpoivb iuytrpallettezb\"\n",
      "73\t\"inuz sknlh eudklwovbnxpaiuteieg ctdeirzrtszaksatamsf heshjdatimucuashjbse\"\n",
      "68\t\"united states pharmacopeia ultraviolet unfallverhiitungsvorschriften\"\n",
      "67\t\"disseminated encephalomyelitis acute disseminated encephalomyelitis\"\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python longest5gram.py -r hadoop hdfs://{TEST_20} \\\n",
    "    --jobconf stream.num.map.output.key.fields=2 \\\n",
    "    --jobconf mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    --jobconf mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    --jobconf mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    --jobconf mapreduce.job.reduces=1 \\\n",
    "    --jobconf partitioner=org.apache.Hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_20.txt\n",
    "!head -n 5 test_20.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 08:45:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_full' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/longest5gram.daghan.20180307.084542.062449\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/longest5gram.daghan.20180307.084542.062449/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1548233626095109079.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1509050304403_27712\n",
      "  Submitted application application_1509050304403_27712\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27712/\n",
      "  Running job: job_1509050304403_27712\n",
      "  Job job_1509050304403_27712 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 36%\n",
      "   map 100% reduce 37%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 39%\n",
      "   map 100% reduce 41%\n",
      "   map 100% reduce 42%\n",
      "   map 100% reduce 43%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 46%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 49%\n",
      "   map 100% reduce 51%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 53%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 56%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 59%\n",
      "   map 100% reduce 60%\n",
      "   map 100% reduce 62%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 64%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_full\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python longest5gram.py -r hadoop hdfs://{FULL_DATA} \\\n",
    "    --jobconf stream.num.map.output.key.fields=2 \\\n",
    "    --jobconf mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    --jobconf mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    --jobconf mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    --jobconf mapreduce.job.reduces=1 \\\n",
    "    --jobconf partitioner=org.apache.Hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_full.txt\n",
    "!head -n 5 test_full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\t\"aiopjumrxuyvaslyhypsibemapodikr ufrydiuuolbigasuaurusrexlisnaye rnoondqsrunsubunougrabberyairtc utahraptoredileipmilbdummyuveri syevrahvelocyallosauruslinrotsr\"\r\n",
      "159\t\"roplezimpredastrodonbraslpklson yhroaclmparcheyxmmioudavesaurus piofpilocowersuruasogetsesnegcp tyravopsifengoquapialloboskenuo owinfuyaiokenecksasxhyilpoynuat\"\r\n",
      "128\t\"rnoondqsrunsubunougrabberyairtc utahraptoredileipmilbdummyuveri syevrahvelocyallosauruslinrotsr jddumhuyparicolevtyps ilonbunurt\"\r\n",
      "119\t\"rtyearthquakecvbnmkdsaw vbnevwvolcaniceruptions floodscvbeavalanchesuyt vbnhurricanestornadoesx tidalwavecvbncyclonecve\"\r\n",
      "119\t\"shipwreckiertgbvcxwqxce cccvbnwseswdfgfiresnplm wqrailroadwrecksutrfhjk explosionstrpiqurecedfg rtyearthquakecvbnmkdsaw\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 test_full.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5grams MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 107.0s ~= 2 minutes  \n",
    "    Reduce tasks = 16 \n",
    "    \n",
    "__Step 2:__   \n",
    "\n",
    "    RUNNING for 108.8s ~= 2 minutes\n",
    "    Reduce tasks = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.B\n",
    "    def mapper1(self, _, line):\n",
    "        ngram, count, p_count, b_count = line.strip().lower().split('\\t')\n",
    "        words = ngram.split(\" \")\n",
    "        for word in words:\n",
    "            yield word, int(count)\n",
    "            \n",
    "    def combiner1(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "            \n",
    "    def reducer1(self, word, counts):\n",
    "        yield word, sum(counts) \n",
    "        \n",
    "    def reducer2(self, word, counts):\n",
    "        yield word, sum(counts) \n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP_TWO = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf={'mapreduce.job.reduces':'4'},\n",
    "                   mapper=self.mapper1,\n",
    "                   combiner=self.combiner1,\n",
    "                   reducer=self.reducer1),\n",
    "            MRStep(reducer=self.reducer2,\n",
    "                   jobconf=JOBCONF_STEP_TWO)]\n",
    "            \n",
    "    \n",
    "    \n",
    "    # END STUDENT CODE 5.6.1.B\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python mostFrequentWords.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > tmp.txt\n",
    "#!cat tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 07:49:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/mostFrequentWords.daghan.20180307.074941.133450\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180307.074941.133450/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9152632569923921711.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_27605\n",
      "  Submitted application application_1509050304403_27605\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27605/\n",
      "  Running job: job_1509050304403_27605\n",
      "  Job job_1509050304403_27605 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_27605 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180307.074941.133450/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=538130\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=566845\n",
      "\t\tFILE: Number of bytes written=1999301\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489801\n",
      "\t\tHDFS: Number of bytes written=538130\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=77348352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=49249280\n",
      "\t\tTotal time spent by all map tasks (ms)=50357\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=151071\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19238\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=96190\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50357\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=19238\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=43350\n",
      "\t\tCombine input records=1558070\n",
      "\t\tCombine output records=54102\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=591\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=18210060\n",
      "\t\tMap output materialized bytes=638588\n",
      "\t\tMap output records=1558070\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tPhysical memory (bytes) snapshot=2848636928\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=54102\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=638588\n",
      "\t\tShuffled Maps =8\n",
      "\t\tSpilled Records=108204\n",
      "\t\tTotal committed heap usage (bytes)=3565158400\n",
      "\t\tVirtual memory (bytes) snapshot=25123209216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob892574562521341675.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 4\n",
      "  number of splits:4\n",
      "  Submitting tokens for job: job_1509050304403_27609\n",
      "  Submitted application application_1509050304403_27609\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27609/\n",
      "  Running job: job_1509050304403_27609\n",
      "  Job job_1509050304403_27609 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_27609 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_1\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=538130\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=538130\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=386047\n",
      "\t\tFILE: Number of bytes written=1455178\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=538874\n",
      "\t\tHDFS: Number of bytes written=538130\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=38163456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10383360\n",
      "\t\tTotal time spent by all map tasks (ms)=24846\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=74538\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4056\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20280\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=24846\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4056\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=12080\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=561\n",
      "\t\tInput split bytes=744\n",
      "\t\tMap input records=36353\n",
      "\t\tMap output bytes=574483\n",
      "\t\tMap output materialized bytes=407796\n",
      "\t\tMap output records=36353\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=3539628032\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=36353\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=407796\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=72706\n",
      "\t\tTotal committed heap usage (bytes)=4012376064\n",
      "\t\tVirtual memory (bytes) snapshot=18260082688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180307.074941.133450...\n",
      "Removing temp directory /tmp/mostFrequentWords.daghan.20180307.074941.133450...\n",
      "\"the\"\t27691943\n",
      "\"of\"\t18590950\n",
      "\"to\"\t11601757\n",
      "\"in\"\t7470912\n",
      "\"a\"\t6926743\n",
      "\"and\"\t6150529\n",
      "\"that\"\t4077421\n",
      "\"is\"\t4074864\n",
      "\"be\"\t3720812\n",
      "\"was\"\t2492074\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mostFrequentWords.py -r hadoop hdfs://{TEST_1} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_1.B.txt\n",
    "!head -n 10 test_1.B.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 06:20:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_20' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/mostFrequentWords.daghan.20180308.062020.732961\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180308.062020.732961/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4744428983864730289.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1509050304403_30511\n",
      "  Submitted application application_1509050304403_30511\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30511/\n",
      "  Running job: job_1509050304403_30511\n",
      "  Job job_1509050304403_30511 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30511 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180308.062020.732961/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=217918535\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1734003\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5216999\n",
      "\t\tFILE: Number of bytes written=16587821\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217921825\n",
      "\t\tHDFS: Number of bytes written=1734003\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=72\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=6\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=26\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=25\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4451002368\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=132669440\n",
      "\t\tTotal time spent by all map tasks (ms)=2897788\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8693364\n",
      "\t\tTotal time spent by all reduce tasks (ms)=51824\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=259120\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2897788\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=51824\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1146400\n",
      "\t\tCombine input records=29656535\n",
      "\t\tCombine output records=689495\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13825\n",
      "\t\tInput split bytes=3290\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=346696368\n",
      "\t\tMap output materialized bytes=8193744\n",
      "\t\tMap output records=29656535\n",
      "\t\tMerged Map outputs=80\n",
      "\t\tPhysical memory (bytes) snapshot=17576620032\n",
      "\t\tReduce input groups=114101\n",
      "\t\tReduce input records=689495\n",
      "\t\tReduce output records=114101\n",
      "\t\tReduce shuffle bytes=8193744\n",
      "\t\tShuffled Maps =80\n",
      "\t\tSpilled Records=1378990\n",
      "\t\tTotal committed heap usage (bytes)=19766706176\n",
      "\t\tVirtual memory (bytes) snapshot=86780882944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6106926703354503076.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 4\n",
      "  number of splits:4\n",
      "  Submitting tokens for job: job_1509050304403_30520\n",
      "  Submitted application application_1509050304403_30520\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30520/\n",
      "  Running job: job_1509050304403_30520\n",
      "  Job job_1509050304403_30520 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30520 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_20\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1734003\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1734003\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1258405\n",
      "\t\tFILE: Number of bytes written=3235348\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1734747\n",
      "\t\tHDFS: Number of bytes written=1734003\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=27654144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24432640\n",
      "\t\tTotal time spent by all map tasks (ms)=18004\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=54012\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=47720\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18004\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9544\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=14270\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=618\n",
      "\t\tInput split bytes=744\n",
      "\t\tMap input records=114101\n",
      "\t\tMap output bytes=1848104\n",
      "\t\tMap output materialized bytes=1315598\n",
      "\t\tMap output records=114101\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=3550183424\n",
      "\t\tReduce input groups=114101\n",
      "\t\tReduce input records=114101\n",
      "\t\tReduce output records=114101\n",
      "\t\tReduce shuffle bytes=1315598\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=228202\n",
      "\t\tTotal committed heap usage (bytes)=3954704384\n",
      "\t\tVirtual memory (bytes) snapshot=18252271616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/mostFrequentWords.daghan.20180308.062020.732961...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/mostFrequentWords.daghan.20180308.062020.732961...\n",
      "\"the\"\t558791097\n",
      "\"of\"\t374964374\n",
      "\"to\"\t222326516\n",
      "\"in\"\t144210167\n",
      "\"a\"\t136711702\n",
      "\"and\"\t115463203\n",
      "\"that\"\t80674504\n",
      "\"is\"\t79028900\n",
      "\"be\"\t68741033\n",
      "\"as\"\t49852316\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mostFrequentWords.py -r hadoop hdfs://{TEST_20} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_20.B.txt\n",
    "!head -n 10 test_20.B.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t558791097\r\n",
      "\"of\"\t374964374\r\n",
      "\"to\"\t222326516\r\n",
      "\"in\"\t144210167\r\n",
      "\"a\"\t136711702\r\n",
      "\"and\"\t115463203\r\n",
      "\"that\"\t80674504\r\n",
      "\"is\"\t79028900\r\n",
      "\"be\"\t68741033\r\n",
      "\"as\"\t49852316\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 test_20.B.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_full\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mostFrequentWords.py -r hadoop hdfs://{FULL_DATA} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_full.B.txt\n",
    "!head -n 10 test_full.B.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words MR stats\n",
    "    \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 590.7s ~= 10 minutes   \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=57   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 76.6s   \n",
    "    Launched map tasks=110\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.C\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        ngram, count, p_count, b_count = line.strip().lower().split('\\t')\n",
    "        words = ngram.split(\" \")\n",
    "        for word in words:\n",
    "            yield word, (int(count), int(p_count))\n",
    "\n",
    "    def combiner1(self, word, values):\n",
    "        total_count = 0\n",
    "        total_p_count = 0\n",
    "        for value in values:\n",
    "            total_count += value[0]\n",
    "            total_p_count += value[1]\n",
    "        yield word, (total_count, total_p_count)\n",
    "        \n",
    "    def reducer1(self, word, values):\n",
    "        total_count = 0\n",
    "        total_p_count = 0\n",
    "        for value in values:\n",
    "            total_count += value[0]\n",
    "            total_p_count += value[1]\n",
    "        yield word, (total_count / total_p_count)\n",
    "        \n",
    "    def reducer2(self, word, values):\n",
    "        for value in values:\n",
    "            yield word, value\n",
    "    \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(jobconf={'mapreduce.job.reduces':'4'},\n",
    "                   mapper=self.mapper1,\n",
    "                   combiner=self.combiner1,\n",
    "                   reducer=self.reducer1),\n",
    "            MRStep(reducer=self.reducer2,\n",
    "                   jobconf={'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'stream.map.output.field.separator':'\\t',\n",
    "                            'mapreduce.partition.keycomparator.options': '-k2,2nr k1,1',\n",
    "                            'mapreduce.job.reduces': '1'\n",
    "                           })\n",
    "        ]\n",
    "    # END STUDENT CODE 5.6.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostLeastDenseWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/mostLeastDenseWords.daghan.20180307.081020.061015\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mostLeastDenseWords.daghan.20180307.081020.061015/output...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.daghan.20180307.081020.061015...\n",
      "\"of\"\t1.0348004094\n",
      "\"general\"\t1.0222222222\n",
      "\"in\"\t1.0326741187\n",
      "\"female\"\t1.0\n",
      "\"for\"\t1.0\n",
      "\"christmas\"\t1.0358152686\n",
      "\"religious\"\t1.0\n",
      "\"bill\"\t1.0\n",
      "\"biography\"\t1.0222222222\n",
      "\"limited\"\t1.0\n",
      "\"narrative\"\t1.0\n",
      "\"forms\"\t1.1262135922\n",
      "\"circumstantial\"\t1.0\n",
      "\"by\"\t1.0333333333\n",
      "\"wales\"\t1.0358152686\n",
      "\"fairy\"\t1.0512820513\n",
      "\"city\"\t1.0333333333\n",
      "\"case\"\t1.0\n",
      "\"child's\"\t1.0358152686\n",
      "\"george\"\t1.0222222222\n",
      "\"the\"\t1.0163934426\n",
      "\"study\"\t1.0\n",
      "\"tales\"\t1.0512820513\n",
      "\"a\"\t1.0282931354\n",
      "\"government\"\t1.0\n",
      "\"collection\"\t1.0863636364\n",
      "\"establishing\"\t1.0\n",
      "\"sea\"\t1.0333333333\n"
     ]
    }
   ],
   "source": [
    "!python mostLeastDenseWords.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > tmp.txt\n",
    "!cat tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 08:11:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/mostLeastDenseWords.daghan.20180307.081127.762697\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.081127.762697/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4041317027189795404.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_27648\n",
      "  Submitted application application_1509050304403_27648\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27648/\n",
      "  Running job: job_1509050304403_27648\n",
      "  Job job_1509050304403_27648 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_27648 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.081127.762697/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=784083\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=756781\n",
      "\t\tFILE: Number of bytes written=2384974\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489801\n",
      "\t\tHDFS: Number of bytes written=784083\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=90342912\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=86202880\n",
      "\t\tTotal time spent by all map tasks (ms)=58817\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=176451\n",
      "\t\tTotal time spent by all reduce tasks (ms)=33673\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=168365\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=58817\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=33673\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=58910\n",
      "\t\tCombine input records=1558070\n",
      "\t\tCombine output records=54102\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1189\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=28088140\n",
      "\t\tMap output materialized bytes=834145\n",
      "\t\tMap output records=1558070\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tPhysical memory (bytes) snapshot=2830426112\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=54102\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=834145\n",
      "\t\tShuffled Maps =8\n",
      "\t\tSpilled Records=108204\n",
      "\t\tTotal committed heap usage (bytes)=3724541952\n",
      "\t\tVirtual memory (bytes) snapshot=25121382400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7489701622120822943.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 4\n",
      "  number of splits:4\n",
      "  Submitting tokens for job: job_1509050304403_27649\n",
      "  Submitted application application_1509050304403_27649\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_27649/\n",
      "  Running job: job_1509050304403_27649\n",
      "  Job job_1509050304403_27649 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_27649 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_1\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=784083\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=784083\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=471034\n",
      "\t\tFILE: Number of bytes written=1626425\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=784835\n",
      "\t\tHDFS: Number of bytes written=784083\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=29898240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14338560\n",
      "\t\tTotal time spent by all map tasks (ms)=19465\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=58395\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5601\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28005\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19465\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5601\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=12650\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=950\n",
      "\t\tInput split bytes=752\n",
      "\t\tMap input records=36353\n",
      "\t\tMap output bytes=820436\n",
      "\t\tMap output materialized bytes=493921\n",
      "\t\tMap output records=36353\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=3524386816\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=36353\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=493921\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=72706\n",
      "\t\tTotal committed heap usage (bytes)=4026531840\n",
      "\t\tVirtual memory (bytes) snapshot=18268426240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.081127.762697...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.daghan.20180307.081127.762697...\n",
      "\"lak\"\t3.072289156626506\n",
      "\"operand\"\t2.353448275862069\n",
      "\"bust\"\t2.3493975903614457\n",
      "\"houseless\"\t2.274891774891775\n",
      "\"gynecological\"\t2.2481536189069424\n",
      "\"denatured\"\t2.1864406779661016\n",
      "\"expiration\"\t2.1568513119533526\n",
      "\"phe\"\t2.0408163265306123\n",
      "\"kiowa\"\t2.0\n",
      "\"apiece\"\t1.9607843137254901\n",
      "\"abolitionists\"\t1.0\n",
      "\"abler\"\t1.0\n",
      "\"abeyance\"\t1.0\n",
      "\"aberdeen\"\t1.0\n",
      "\"abed\"\t1.0\n",
      "\"abdication\"\t1.0\n",
      "\"abbotsford\"\t1.0\n",
      "\"abbe\"\t1.0\n",
      "\"aback\"\t1.0\n",
      "\"urchins\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs://{TEST_1} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_1.C.txt\n",
    "!head -n 10 test_1.C.txt\n",
    "!tail -n 10 test_1.C.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/daghan/hw5/test_20': No such file or directory\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/mostLeastDenseWords.daghan.20180307.162540.018341\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.162540.018341/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6406184891762809544.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1509050304403_28587\n",
      "  Submitted application application_1509050304403_28587\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_28587/\n",
      "  Running job: job_1509050304403_28587\n",
      "  Job job_1509050304403_28587 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_28587 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.162540.018341/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=217918535\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2565464\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7488745\n",
      "\t\tFILE: Number of bytes written=21398429\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217921825\n",
      "\t\tHDFS: Number of bytes written=2565464\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=72\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=11\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=31\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=30\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5135158272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=121441280\n",
      "\t\tTotal time spent by all map tasks (ms)=3343202\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10029606\n",
      "\t\tTotal time spent by all reduce tasks (ms)=47438\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=237190\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3343202\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=47438\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1361910\n",
      "\t\tCombine input records=29656535\n",
      "\t\tCombine output records=689495\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=15597\n",
      "\t\tInput split bytes=3290\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=534732323\n",
      "\t\tMap output materialized bytes=10731886\n",
      "\t\tMap output records=29656535\n",
      "\t\tMerged Map outputs=80\n",
      "\t\tPhysical memory (bytes) snapshot=17542873088\n",
      "\t\tReduce input groups=114101\n",
      "\t\tReduce input records=689495\n",
      "\t\tReduce output records=114101\n",
      "\t\tReduce shuffle bytes=10731886\n",
      "\t\tShuffled Maps =80\n",
      "\t\tSpilled Records=1378990\n",
      "\t\tTotal committed heap usage (bytes)=19495124992\n",
      "\t\tVirtual memory (bytes) snapshot=86746300416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2580462045353992554.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 4\n",
      "  number of splits:4\n",
      "  Submitting tokens for job: job_1509050304403_28596\n",
      "  Submitted application application_1509050304403_28596\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_28596/\n",
      "  Running job: job_1509050304403_28596\n",
      "  Job job_1509050304403_28596 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_28596 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/test_20\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2565464\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2565464\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1528550\n",
      "\t\tFILE: Number of bytes written=3795153\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2566216\n",
      "\t\tHDFS: Number of bytes written=2565464\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=88137216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=37376000\n",
      "\t\tTotal time spent by all map tasks (ms)=57381\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=172143\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14600\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=73000\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=57381\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14600\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=22450\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1663\n",
      "\t\tInput split bytes=752\n",
      "\t\tMap input records=114101\n",
      "\t\tMap output bytes=2679565\n",
      "\t\tMap output materialized bytes=1605128\n",
      "\t\tMap output records=114101\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=3553153024\n",
      "\t\tReduce input groups=114101\n",
      "\t\tReduce input records=114101\n",
      "\t\tReduce output records=114101\n",
      "\t\tReduce shuffle bytes=1605128\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=228202\n",
      "\t\tTotal committed heap usage (bytes)=4049076224\n",
      "\t\tVirtual memory (bytes) snapshot=18259259392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/mostLeastDenseWords.daghan.20180307.162540.018341...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/mostLeastDenseWords.daghan.20180307.162540.018341...\n",
      "\"nn\"\t6.606896551724138\n",
      "\"llll\"\t4.511627906976744\n",
      "\"vocht\"\t3.8902147971360383\n",
      "\"pfeffermann\"\t3.576923076923077\n",
      "\"madarassy\"\t3.576923076923077\n",
      "\"pogue\"\t3.4066265060240966\n",
      "\"lak\"\t3.072289156626506\n",
      "\"bl\"\t3.019280205655527\n",
      "\"nonsquamous\"\t2.81981981981982\n",
      "\"nonmorular\"\t2.81981981981982\n",
      "\"zuleikah\"\t1.0\n",
      "\"zulestein\"\t1.0\n",
      "\"zumindest\"\t1.0\n",
      "\"zurito\"\t1.0\n",
      "\"zustimmung\"\t1.0\n",
      "\"aarne\"\t1.0\n",
      "\"zwicker\"\t1.0\n",
      "\"zxcvframeqasfuc\"\t1.0\n",
      "\"zygomaticus\"\t1.0\n",
      "\"zymotic\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs://{TEST_20} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_20.C.txt\n",
    "!head -n 10 test_20.C.txt\n",
    "!tail -n 10 test_20.C.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 649.2s  ~= 10 minutes      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=57     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 74.4s  ~= 1 minute    \n",
    "    Launched map tasks=110   \n",
    "    Launched reduce tasks=20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.D\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        ngram, count, p_count, b_count = line.strip().lower().split('\\t')\n",
    "        yield len(ngram), int(count)\n",
    "\n",
    "    def combiner1(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "        \n",
    "    def reducer1(self, key, values):\n",
    "        yield key, sum(values)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   combiner=self.combiner1,\n",
    "                   reducer=self.reducer1,\n",
    "                   jobconf={'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'stream.map.output.field.separator':'\\t',\n",
    "                            'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "                            'mapreduce.job.reduces': '1'\n",
    "                           })\n",
    "        ]\n",
    "    \n",
    "    # END STUDENT CODE 5.6.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    distribution.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/distribution.daghan.20180307.173644.858328\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/distribution.daghan.20180307.173644.858328/output...\n",
      "Removing temp directory /tmp/distribution.daghan.20180307.173644.858328...\n",
      "26\t102\n",
      "22\t447\n",
      "27\t123\n",
      "23\t55\n",
      "28\t1099\n",
      "24\t116\n",
      "17\t62\n",
      "29\t92\n",
      "33\t121\n"
     ]
    }
   ],
   "source": [
    "!python distribution.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > tmp.txt\n",
    "!cat tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 17:36:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/5.6distributions' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/distribution.daghan.20180307.173648.392661\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/distribution.daghan.20180307.173648.392661/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6873644724684000304.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_28753\n",
      "  Submitted application application_1509050304403_28753\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_28753/\n",
      "  Running job: job_1509050304403_28753\n",
      "  Job job_1509050304403_28753 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_28753 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/5.6distributions/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=451\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=889\n",
      "\t\tFILE: Number of bytes written=399903\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489801\n",
      "\t\tHDFS: Number of bytes written=451\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=92938752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15173120\n",
      "\t\tTotal time spent by all map tasks (ms)=60507\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=181521\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5927\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29635\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=60507\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5927\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=21380\n",
      "\t\tCombine input records=311614\n",
      "\t\tCombine output records=92\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=546\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=2288215\n",
      "\t\tMap output materialized bytes=972\n",
      "\t\tMap output records=311614\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1924534272\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=92\n",
      "\t\tReduce output records=49\n",
      "\t\tReduce shuffle bytes=972\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=184\n",
      "\t\tTotal committed heap usage (bytes)=2188378112\n",
      "\t\tVirtual memory (bytes) snapshot=11412353024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/distribution.daghan.20180307.173648.392661...\n",
      "Removing temp directory /tmp/distribution.daghan.20180307.173648.392661...\n",
      "58\t95\n",
      "57\t142\n",
      "55\t680\n",
      "54\t166\n",
      "53\t977\n",
      "52\t1926\n",
      "51\t725\n",
      "50\t941\n",
      "49\t3262\n",
      "48\t4746\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"5.6distributions/\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python distribution.py -r hadoop hdfs://{TEST_1} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_1.D.txt\n",
    "!head -n 10 test_1.D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 17:39:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/5.6distributions' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/distribution.daghan.20180307.173939.450382\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/distribution.daghan.20180307.173939.450382/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1803198631576738858.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1509050304403_28755\n",
      "  Submitted application application_1509050304403_28755\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_28755/\n",
      "  Running job: job_1509050304403_28755\n",
      "  Job job_1509050304403_28755 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_28755 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/5.6distributions/\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=217918535\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=594\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6953\n",
      "\t\tFILE: Number of bytes written=2803490\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217921825\n",
      "\t\tHDFS: Number of bytes written=594\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=63\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=21\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=20\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=953221632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7910400\n",
      "\t\tTotal time spent by all map tasks (ms)=620587\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1861761\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3090\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15450\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=620587\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3090\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=307050\n",
      "\t\tCombine input records=5931307\n",
      "\t\tCombine output records=929\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=6628\n",
      "\t\tInput split bytes=3290\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=43557174\n",
      "\t\tMap output materialized bytes=9840\n",
      "\t\tMap output records=5931307\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tPhysical memory (bytes) snapshot=16414507008\n",
      "\t\tReduce input groups=62\n",
      "\t\tReduce input records=929\n",
      "\t\tReduce output records=62\n",
      "\t\tReduce shuffle bytes=9840\n",
      "\t\tShuffled Maps =20\n",
      "\t\tSpilled Records=1858\n",
      "\t\tTotal committed heap usage (bytes)=18282971136\n",
      "\t\tVirtual memory (bytes) snapshot=73002020864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/distribution.daghan.20180307.173939.450382...\n",
      "Removing temp directory /tmp/distribution.daghan.20180307.173939.450382...\n",
      "83\t51\n",
      "79\t83\n",
      "73\t91\n",
      "68\t44\n",
      "67\t88\n",
      "65\t92\n",
      "64\t47\n",
      "63\t60\n",
      "62\t233\n",
      "61\t734\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"5.6distributions/\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python distribution.py -r hadoop hdfs://{TEST_20} \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_20.D.txt\n",
    "!head -n 10 test_20.D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\t51\r\n",
      "79\t83\r\n",
      "73\t91\r\n",
      "68\t44\r\n",
      "67\t88\r\n",
      "65\t92\r\n",
      "64\t47\r\n",
      "63\t60\r\n",
      "62\t233\r\n",
      "61\t734\r\n",
      "60\t375\r\n",
      "59\t988\r\n",
      "58\t1396\r\n",
      "57\t1731\r\n",
      "56\t2162\r\n",
      "55\t6753\r\n",
      "54\t8236\r\n",
      "53\t8418\r\n",
      "52\t17363\r\n",
      "51\t18140\r\n",
      "50\t35439\r\n",
      "49\t55450\r\n",
      "48\t80287\r\n",
      "47\t114710\r\n",
      "46\t171016\r\n",
      "45\t259583\r\n",
      "44\t342105\r\n",
      "43\t533046\r\n",
      "42\t753177\r\n",
      "41\t1102899\r\n",
      "40\t1525514\r\n",
      "39\t2313687\r\n",
      "38\t3127627\r\n",
      "37\t4649747\r\n",
      "36\t6146149\r\n",
      "35\t8922389\r\n",
      "34\t11635886\r\n",
      "33\t15311428\r\n",
      "32\t21052427\r\n",
      "31\t26894155\r\n",
      "30\t34263002\r\n",
      "29\t43577723\r\n",
      "28\t53285299\r\n",
      "27\t64822041\r\n",
      "26\t73556536\r\n",
      "25\t83732548\r\n",
      "24\t90851070\r\n",
      "23\t92561326\r\n",
      "22\t90993712\r\n",
      "21\t82676663\r\n",
      "20\t69766167\r\n",
      "19\t55970323\r\n",
      "18\t33170673\r\n",
      "17\t16739704\r\n",
      "16\t6591709\r\n",
      "15\t2320575\r\n",
      "14\t545595\r\n",
      "13\t74798\r\n",
      "12\t11766\r\n",
      "11\t1241\r\n",
      "10\t343\r\n",
      "9\t1518\r\n"
     ]
    }
   ],
   "source": [
    "!cat test_20.D.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution MRJob stats\n",
    "\n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 157.8s ~= 2.6 minutes  \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=16   \n",
    "    \n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 115.0s ~= 2 minutes   \n",
    "    Launched map tasks=139\n",
    "\tLaunched reduce tasks=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5644bd050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"test_20.D.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.2 - OPTIONAL: log-log plots (PHASE 2)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.7 - Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3-5 plus some preprocessing)   \n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis (see stopWords in the cell below)\n",
    "\n",
    "__A large subset of the Google n-grams dataset as was described above__\n",
    "\n",
    "For each HW 5.6 - 5.7.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "              'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.1 Running on 1 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out stop words from the popular words list (which we have computed in 5.6.1B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting filterStopWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile filterStopWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class filterStopWords(MRJob):\n",
    "    stopwords =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "              'don', 'should', 'now']\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.D\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        word, count = line.strip('\\'\"').split('\\t')\n",
    "        word = word.strip('\\'\"')\n",
    "        if not(word in self.stopwords):\n",
    "            yield word, int(count)\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        for value in values:\n",
    "            yield key, value\n",
    "   \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   reducer=self.reducer1,\n",
    "                   jobconf={'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'stream.map.output.field.separator':'\\t',\n",
    "                            'mapreduce.partition.keycomparator.options': '-k2nr -k1',\n",
    "                            'mapreduce.job.reduces': '1'\n",
    "                           })\n",
    "        ]\n",
    "    \n",
    "    # END STUDENT CODE 5.6.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    filterStopWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/filterStopWords.daghan.20180307.203235.182762\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/filterStopWords.daghan.20180307.203235.182762/output...\n",
      "Removing temp directory /tmp/filterStopWords.daghan.20180307.203235.182762...\n",
      "\"spontaneously\"\t1098\n",
      "\"spooky\"\t57\n",
      "\"spoon\"\t330\n",
      "\"spooned\"\t54\n",
      "\"spoonful\"\t255\n",
      "\"spoonfuls\"\t129\n",
      "\"spoons\"\t173\n",
      "\"sporadic\"\t414\n",
      "\"spores\"\t116\n",
      "\"sporophyte\"\t56\n"
     ]
    }
   ],
   "source": [
    "!python filterStopWords.py -r local test_1.B.txt > tmp.txt\n",
    "!head -n 10 tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/07 20:32:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/5.7.1_test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/filterStopWords.daghan.20180307.203243.832928\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/filterStopWords.daghan.20180307.203243.832928/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6800670787857007240.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_29059\n",
      "  Submitted application application_1509050304403_29059\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_29059/\n",
      "  Running job: job_1509050304403_29059\n",
      "  Job job_1509050304403_29059 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_29059 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_test_1\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=662281\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=536373\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=380519\n",
      "\t\tFILE: Number of bytes written=1158458\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=662629\n",
      "\t\tHDFS: Number of bytes written=536373\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13000704\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22215680\n",
      "\t\tTotal time spent by all map tasks (ms)=8464\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25392\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8678\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43390\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8464\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8678\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7690\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=368\n",
      "\t\tInput split bytes=348\n",
      "\t\tMap input records=36353\n",
      "\t\tMap output bytes=572599\n",
      "\t\tMap output materialized bytes=380869\n",
      "\t\tMap output records=36226\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1914810368\n",
      "\t\tReduce input groups=36226\n",
      "\t\tReduce input records=36226\n",
      "\t\tReduce output records=36226\n",
      "\t\tReduce shuffle bytes=380869\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=72452\n",
      "\t\tTotal committed heap usage (bytes)=2253389824\n",
      "\t\tVirtual memory (bytes) snapshot=11412992000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/filterStopWords.daghan.20180307.203243.832928...\n",
      "Removing temp directory /tmp/filterStopWords.daghan.20180307.203243.832928...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python filterStopWords.py -r  hadoop test_1.B.txt \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1.test_1.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10000 test_5.7.1.test_1.txt > test_5.7.1.test_1_top_10K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes2.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "class buildStripes2(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol # Not strictly necessary, but the output is prettier this way\n",
    "\n",
    "\n",
    "    n_words = []\n",
    "    k_features = []\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #with open(\"test_5.7.1.test_1_top_10K.txt\", \"r\") as IF:\n",
    "        with open(\"test_5.7.1.test_20_top_10K.txt\", \"r\") as IF:\n",
    "            for line in IF:\n",
    "                word, count = line.strip().split(\"\\t\")\n",
    "                word = word.strip(\"\\\"\")\n",
    "                self.n_words.append(word)\n",
    "        self.k_features = self.n_words[9000:]\n",
    "\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        words, count, pages_count, books_count = line.strip().split(\"\\t\")\n",
    "        words = words.lower().split(\" \")\n",
    "        for word1 in words:\n",
    "            if (word1 in self.n_words):\n",
    "                stripe = defaultdict(int)\n",
    "                for word2 in words:\n",
    "                    #if (word2 in self.k_features):\n",
    "                    #    print(word2 + \" is in the features list\")\n",
    "                    if ((word1 != word2) & (word2 in self.k_features)):\n",
    "                        stripe[word2] = int(count)\n",
    "                if (stripe):\n",
    "                    yield word1, stripe\n",
    "\n",
    "    def reducer(self, word, stripes):\n",
    "        merged_stripe = {}\n",
    "        for stripe in stripes:\n",
    "            merged_stripe.update(stripe)\n",
    "        yield word, merged_stripe\n",
    "        \n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                    mapper_init = self.mapper_init,\n",
    "                    mapper = self.mapper,\n",
    "                    reducer = self.reducer)]\n",
    "                    #reducer_final = self.reducer_final)]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    buildStripes2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/buildStripes2.daghan.20180308.044609.810554\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/buildStripes2.daghan.20180308.044609.810554/output...\n",
      "Removing temp directory /tmp/buildStripes2.daghan.20180308.044609.810554...\n",
      "\"number\"\t{\"binary\":89,\"pamphlets\":166}\n",
      "\"large\"\t{\"binary\":89,\"pamphlets\":166}\n",
      "\"marrow\"\t{\"aspiration\":94}\n",
      "\"done\"\t{\"secretly\":84}\n",
      "\"belgium\"\t{\"albert\":46}\n",
      "\"opera\"\t{\"extant\":47}\n",
      "\"agriculture\"\t{\"iowa\":111}\n",
      "\"need\"\t{\"banker\":84}\n",
      "\"effort\"\t{\"cooperative\":81}\n",
      "\"bone\"\t{\"aspiration\":94}\n",
      "\"allied\"\t{\"sovereigns\":72}\n",
      "\"value\"\t{\"inestimable\":57}\n",
      "\"common\"\t{\"trick\":78}\n",
      "\"clinical\"\t{\"entity\":127}\n",
      "\"along\"\t{\"peru\":206}\n",
      "\"long\"\t{\"reflecting\":52}\n",
      "\"university\"\t{\"iowa\":111}\n",
      "\"state\"\t{\"iowa\":111}\n",
      "\"work\"\t{\"inestimable\":57}\n",
      "\"age\"\t{\"franklin\":77}\n",
      "\"man\"\t{\"pit\":74}\n",
      "\"manual\"\t{\"manipulation\":131}\n",
      "\"coast\"\t{\"peru\":206}\n",
      "\"commentary\"\t{\"apology\":110}\n",
      "\"recent\"\t{\"albert\":83}\n"
     ]
    }
   ],
   "source": [
    "!python buildStripes2.py googlebooks-eng-all-5gram-20090715-0-filtered-first-1000-lines.txt \\\n",
    "    -r local --file test_5.7.1.test_1_top_10K.txt > tmp.txt\n",
    "!cat tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/daghan/hw5/5.7.1_stripes_test_1': No such file or directory\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes2.daghan.20180308.045114.381500\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/buildStripes2.daghan.20180308.045114.381500/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7748854086798717511.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30273\n",
      "  Submitted application application_1509050304403_30273\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30273/\n",
      "  Running job: job_1509050304403_30273\n",
      "  Job job_1509050304403_30273 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30273 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_stripes_test_1\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11489475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=181193\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=143747\n",
      "\t\tFILE: Number of bytes written=686886\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11489801\n",
      "\t\tHDFS: Number of bytes written=181193\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=463105536\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=37995520\n",
      "\t\tTotal time spent by all map tasks (ms)=301501\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=904503\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14842\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=74210\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=301501\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14842\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=251360\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=619\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=242031\n",
      "\t\tMap output materialized bytes=147695\n",
      "\t\tMap output records=9177\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1913159680\n",
      "\t\tReduce input groups=3686\n",
      "\t\tReduce input records=9177\n",
      "\t\tReduce output records=3686\n",
      "\t\tReduce shuffle bytes=147695\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18354\n",
      "\t\tTotal committed heap usage (bytes)=2254962688\n",
      "\t\tVirtual memory (bytes) snapshot=11412086784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/buildStripes2.daghan.20180308.045114.381500...\n",
      "Removing temp directory /tmp/buildStripes2.daghan.20180308.045114.381500...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_stripes_test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python buildStripes2.py hdfs://{TEST_1} \\\n",
    "    -r hadoop --file test_5.7.1.test_1_top_10K.txt \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_stripes_test_1.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"abandonment\"\t{\"blockade\": 52, \"virtual\": 40}\r\n",
      "\"abdomen\"\t{\"flank\": 124, \"distention\": 42}\r\n",
      "\"abdominal\"\t{\"abrupt\": 42}\r\n",
      "\"abilities\"\t{\"destitute\": 89}\r\n",
      "\"ability\"\t{\"retrieve\": 94, \"discriminate\": 72, \"dramatically\": 82, \"inspire\": 41}\r\n",
      "\"able\"\t{\"locate\": 61, \"accustom\": 270, \"illuminate\": 67, \"impatience\": 65, \"unlimited\": 65, \"utilize\": 53, \"packet\": 59, \"composer\": 120, \"dictate\": 101, \"retrieve\": 332, \"crawl\": 51, \"afterward\": 138, \"rejoiced\": 46}\r\n",
      "\"abnormalities\"\t{\"reproductive\": 154}\r\n",
      "\"abroad\"\t{\"fountain\": 53}\r\n",
      "\"absence\"\t{\"rainfall\": 76, \"receptors\": 43, \"bacilli\": 468}\r\n",
      "\"absent\"\t{\"spectacle\": 129}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 test_5.7.1_stripes_test_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/invertedIndex.daghan.20180308.055136.431800\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.daghan.20180308.055136.431800/output...\n",
      "Removing temp directory /tmp/invertedIndex.daghan.20180308.055136.431800...\n",
      "\"speedily\"\t[[\"hidden\",4],[\"irresistible\",1],[\"joined\",4],[\"lost\",4],[\"might\",18],[\"practice\",5],[\"recalled\",1],[\"reduced\",2],[\"reinforcement\",1],[\"secured\",2],[\"whose\",2]]\n",
      "\"spinning\"\t[[\"across\",8],[\"cotton\",3],[\"development\",13],[\"employed\",2],[\"everything\",1],[\"must\",25],[\"opposite\",5],[\"room\",6],[\"seemed\",9],[\"taken\",9],[\"tasks\",1],[\"tea\",1],[\"tradition\",4],[\"went\",10]]\n",
      "\"splendor\"\t[[\"castle\",2],[\"felt\",6],[\"great\",24],[\"live\",3],[\"pomp\",1],[\"quite\",7],[\"scene\",5]]\n",
      "\"spouses\"\t[[\"children\",9],[\"dependent\",1],[\"either\",7],[\"find\",13],[\"less\",6],[\"minor\",2],[\"prevent\",6],[\"soon\",8]]\n",
      "\"spy\"\t[[\"alliance\",3],[\"english\",8],[\"order\",16],[\"special\",2],[\"suspected\",1],[\"things\",9]]\n",
      "\"ss\"\t[[\"dental\",2],[\"development\",13],[\"virgin\",1],[\"white\",7]]\n",
      "\"standardization\"\t[[\"formulation\",1],[\"group\",10],[\"product\",3]]\n",
      "\"stanley\"\t[[\"like\",20],[\"poems\",1],[\"port\",3],[\"thank\",1],[\"thomas\",2],[\"would\",68]]\n",
      "\"steer\"\t[[\"away\",14],[\"clear\",4],[\"effort\",4],[\"enabled\",3],[\"towards\",3],[\"trying\",3],[\"way\",22]]\n",
      "\"sterile\"\t[[\"amount\",11],[\"areas\",6],[\"body\",8],[\"distilled\",1],[\"look\",10],[\"normal\",6],[\"saline\",1],[\"small\",12],[\"water\",16]]\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py test_5.7.1_stripes_test_1.txt -r local  > tmp.txt\n",
    "!head -n 10 tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 05:09:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/5.7.1_inverted_test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex.daghan.20180308.050952.669228\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180308.050952.669228/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1208248808404271351.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30332\n",
      "  Submitted application application_1509050304403_30332\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30332/\n",
      "  Running job: job_1509050304403_30332\n",
      "  Job job_1509050304403_30332 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30332 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_inverted_test_1\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=221669\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=146123\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=97250\n",
      "\t\tFILE: Number of bytes written=591315\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=222049\n",
      "\t\tHDFS: Number of bytes written=146123\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=50916864\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=62474240\n",
      "\t\tTotal time spent by all map tasks (ms)=33149\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=99447\n",
      "\t\tTotal time spent by all reduce tasks (ms)=24404\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=122020\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=33149\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=24404\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5750\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=937\n",
      "\t\tInput split bytes=380\n",
      "\t\tMap input records=3686\n",
      "\t\tMap output bytes=229352\n",
      "\t\tMap output materialized bytes=98513\n",
      "\t\tMap output records=8744\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1911828480\n",
      "\t\tReduce input groups=8744\n",
      "\t\tReduce input records=8744\n",
      "\t\tReduce output records=992\n",
      "\t\tReduce shuffle bytes=98513\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=17488\n",
      "\t\tTotal committed heap usage (bytes)=2213543936\n",
      "\t\tVirtual memory (bytes) snapshot=11412365312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180308.050952.669228...\n",
      "Removing temp directory /tmp/invertedIndex.daghan.20180308.050952.669228...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_inverted_test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python invertedIndex.py test_5.7.1_stripes_test_1.txt -r hadoop \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_inverted_test_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Creating temp directory /tmp/similarity.daghan.20180308.055151.843122\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/similarity.daghan.20180308.055151.843122/output...\n",
      "Removing temp directory /tmp/similarity.daghan.20180308.055151.843122...\n",
      "0.2276787492\t[[\"amount\",\"low\"],0.3015113446,0.1538461538]\n",
      "0.2276787492\t[[\"given\",\"short\"],0.3015113446,0.1538461538]\n",
      "0.2276787492\t[[\"government\",\"issue\"],0.3015113446,0.1538461538]\n",
      "0.2276787492\t[[\"per\",\"ten\"],0.3015113446,0.1538461538]\n",
      "0.2276787492\t[[\"show\",\"yet\"],0.3015113446,0.1538461538]\n",
      "0.2303221146\t[[\"duties\",\"work\"],0.3429971703,0.1176470588]\n",
      "0.2324045318\t[[\"began\",\"hear\"],0.298142397,0.1666666667]\n",
      "0.233113883\t[[\"treatment\",\"used\"],0.316227766,0.15]\n",
      "0.2392766953\t[[\"absolutely\",\"became\"],0.3535533906,0.125]\n",
      "0.2392766953\t[[\"absolutely\",\"become\"],0.3535533906,0.125]\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py test_5.7.1_inverted_test_1.txt -r local  > tmp.txt\n",
    "!head -n 10 tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 05:52:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/5.7.1_similarity_test_1' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity.daghan.20180308.055204.617831\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.055204.617831/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7048224377359100318.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30437\n",
      "  Submitted application application_1509050304403_30437\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30437/\n",
      "  Running job: job_1509050304403_30437\n",
      "  Job job_1509050304403_30437 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30437 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.055204.617831/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204134\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3084934\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=486484\n",
      "\t\tFILE: Number of bytes written=1378271\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=204510\n",
      "\t\tHDFS: Number of bytes written=3084934\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25812480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=35991040\n",
      "\t\tTotal time spent by all map tasks (ms)=16805\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=50415\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14059\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=70295\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16805\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14059\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=11180\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=557\n",
      "\t\tInput split bytes=376\n",
      "\t\tMap input records=992\n",
      "\t\tMap output bytes=1388990\n",
      "\t\tMap output materialized bytes=496232\n",
      "\t\tMap output records=42354\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1910431744\n",
      "\t\tReduce input groups=41301\n",
      "\t\tReduce input records=42354\n",
      "\t\tReduce output records=41301\n",
      "\t\tReduce shuffle bytes=496232\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=84708\n",
      "\t\tTotal committed heap usage (bytes)=2224553984\n",
      "\t\tVirtual memory (bytes) snapshot=11413635072\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4157688489272217476.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30441\n",
      "  Submitted application application_1509050304403_30441\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30441/\n",
      "  Running job: job_1509050304403_30441\n",
      "  Job job_1509050304403_30441 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30441 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_similarity_test_1\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3115331\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3062451\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=589780\n",
      "\t\tFILE: Number of bytes written=1604106\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3115687\n",
      "\t\tHDFS: Number of bytes written=3062451\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10638336\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=33551360\n",
      "\t\tTotal time spent by all map tasks (ms)=6926\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20778\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13106\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=65530\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6926\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13106\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7610\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=406\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=41301\n",
      "\t\tMap output bytes=3126235\n",
      "\t\tMap output materialized bytes=617763\n",
      "\t\tMap output records=41301\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1911410688\n",
      "\t\tReduce input groups=41301\n",
      "\t\tReduce input records=41301\n",
      "\t\tReduce output records=41301\n",
      "\t\tReduce shuffle bytes=617763\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=82602\n",
      "\t\tTotal committed heap usage (bytes)=2243428352\n",
      "\t\tVirtual memory (bytes) snapshot=11413864448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.055204.617831...\n",
      "Removing temp directory /tmp/similarity.daghan.20180308.055204.617831...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_similarity_test_1\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python similarity.py test_5.7.1_inverted_test_1.txt -r hadoop \\\n",
    "    --output-dir={OUTPUT_PATH} \\\n",
    "    --no-output\n",
    "    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_similarity_test_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[[\"abdominal\", \"decisive\"], 1.0, 1.0]\n",
      "1.0\t[[\"abdominal\", \"letting\"], 1.0, 1.0]\n",
      "1.0\t[[\"abdominal\", \"onset\"], 1.0, 1.0]\n",
      "1.0\t[[\"abilities\", \"favorite\"], 1.0, 1.0]\n",
      "1.0\t[[\"abilities\", \"necessaries\"], 1.0, 1.0]\n",
      "1.0\t[[\"abilities\", \"timber\"], 1.0, 1.0]\n",
      "1.0\t[[\"abnormalities\", \"organs\"], 1.0, 1.0]\n",
      "1.0\t[[\"abnormalities\", \"productive\"], 1.0, 1.0]\n",
      "1.0\t[[\"abroad\", \"arising\"], 1.0, 1.0]\n",
      "1.0\t[[\"abroad\", \"roman\"], 1.0, 1.0]\n",
      "1.0\t[[\"abroad\", \"springing\"], 1.0, 1.0]\n",
      "1.0\t[[\"abroad\", \"whence\"], 1.0, 1.0]\n",
      "1.0\t[[\"absent\", \"looking\"], 1.0, 1.0]\n",
      "1.0\t[[\"absolutely\", \"author's\"], 1.0, 1.0]\n",
      "1.0\t[[\"abundant\", \"bundle\"], 1.0, 1.0]\n",
      "1.0\t[[\"abundant\", \"fruits\"], 1.0, 1.0]\n",
      "1.0\t[[\"abundant\", \"thrown\"], 1.0, 1.0]\n",
      "1.0\t[[\"academy\", \"science\"], 1.0, 1.0]\n",
      "1.0\t[[\"acceleration\", \"representing\"], 1.0, 1.0]\n",
      "1.0\t[[\"acceleration\", \"spectrum\"], 1.0, 1.0]\n",
      "1.0\t[[\"acceleration\", \"transform\"], 1.0, 1.0]\n",
      "1.0\t[[\"accept\", \"declaration\"], 1.0, 1.0]\n",
      "1.0\t[[\"accept\", \"democracy\"], 1.0, 1.0]\n",
      "1.0\t[[\"accepts\", \"agrees\"], 1.0, 1.0]\n",
      "1.0\t[[\"accepts\", \"resolutions\"], 1.0, 1.0]\n",
      "1.0\t[[\"accommodate\", \"gap\"], 1.0, 1.0]\n",
      "1.0\t[[\"accommodate\", \"progressive\"], 1.0, 1.0]\n",
      "1.0\t[[\"accommodated\", \"expedition\"], 1.0, 1.0]\n",
      "1.0\t[[\"accommodation\", \"vascular\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompanied\", \"attend\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompanied\", \"ceremonies\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompanied\", \"consecrated\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompanied\", \"honours\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompany\", \"exchequer\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompany\", \"noblest\"], 1.0, 1.0]\n",
      "1.0\t[[\"accompany\", \"princes\"], 1.0, 1.0]\n",
      "1.0\t[[\"accomplish\", \"procedures\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"brown\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"coal\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"courtesy\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"satisfaction\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"surgeon\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounts\", \"warmth\"], 1.0, 1.0]\n",
      "1.0\t[[\"accumulation\", \"inhibitors\"], 1.0, 1.0]\n",
      "1.0\t[[\"accurate\", \"englishman\"], 1.0, 1.0]\n",
      "1.0\t[[\"accurate\", \"ridge\"], 1.0, 1.0]\n",
      "1.0\t[[\"accused\", \"constructed\"], 1.0, 1.0]\n",
      "1.0\t[[\"accused\", \"contrived\"], 1.0, 1.0]\n",
      "1.0\t[[\"accused\", \"examined\"], 1.0, 1.0]\n",
      "1.0\t[[\"accused\", \"ignored\"], 1.0, 1.0]\n",
      "1.0\t[[\"achieved\", \"court's\"], 1.0, 1.0]\n",
      "1.0\t[[\"achieved\", \"decisions\"], 1.0, 1.0]\n",
      "1.0\t[[\"achieved\", \"signs\"], 1.0, 1.0]\n",
      "1.0\t[[\"acknowledgment\", \"economical\"], 1.0, 1.0]\n",
      "1.0\t[[\"acknowledgment\", \"englishmen\"], 1.0, 1.0]\n",
      "1.0\t[[\"acknowledgment\", \"gradual\"], 1.0, 1.0]\n",
      "1.0\t[[\"acknowledgment\", \"granted\"], 1.0, 1.0]\n",
      "1.0\t[[\"acquisition\", \"continued\"], 1.0, 1.0]\n",
      "1.0\t[[\"acquisition\", \"education\"], 1.0, 1.0]\n",
      "1.0\t[[\"acquisition\", \"outset\"], 1.0, 1.0]\n",
      "1.0\t[[\"acquisition\", \"vigorously\"], 1.0, 1.0]\n",
      "1.0\t[[\"acre\", \"inhabited\"], 1.0, 1.0]\n",
      "1.0\t[[\"acted\", \"remained\"], 1.0, 1.0]\n",
      "1.0\t[[\"acted\", \"storms\"], 1.0, 1.0]\n",
      "1.0\t[[\"actually\", \"confine\"], 1.0, 1.0]\n",
      "1.0\t[[\"actually\", \"gladly\"], 1.0, 1.0]\n",
      "1.0\t[[\"acute\", \"doctor\"], 1.0, 1.0]\n",
      "1.0\t[[\"acute\", \"infectious\"], 1.0, 1.0]\n",
      "1.0\t[[\"adapt\", \"environment\"], 1.0, 1.0]\n",
      "1.0\t[[\"adapt\", \"placement\"], 1.0, 1.0]\n",
      "1.0\t[[\"adapted\", \"dream\"], 1.0, 1.0]\n",
      "1.0\t[[\"adapted\", \"resembling\"], 1.0, 1.0]\n",
      "1.0\t[[\"add\", \"creator\"], 1.0, 1.0]\n",
      "1.0\t[[\"add\", \"imprint\"], 1.0, 1.0]\n",
      "1.0\t[[\"add\", \"retain\"], 1.0, 1.0]\n",
      "1.0\t[[\"adding\", \"code\"], 1.0, 1.0]\n",
      "1.0\t[[\"adding\", \"expenditure\"], 1.0, 1.0]\n",
      "1.0\t[[\"adding\", \"involves\"], 1.0, 1.0]\n",
      "1.0\t[[\"adding\", \"occurrence\"], 1.0, 1.0]\n",
      "1.0\t[[\"adding\", \"transfer\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressed\", \"features\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressed\", \"portions\"], 1.0, 1.0]\n",
      "1.0\t[[\"adherence\", \"bloody\"], 1.0, 1.0]\n",
      "1.0\t[[\"adherence\", \"enemies\"], 1.0, 1.0]\n",
      "1.0\t[[\"adherence\", \"fighting\"], 1.0, 1.0]\n",
      "1.0\t[[\"adherence\", \"prejudice\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"confirmation\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"employee\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"employees\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"metallic\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"venous\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrative\", \"issues\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrative\", \"lending\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrative\", \"reorganization\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrator\", \"aims\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrator\", \"skilful\"], 1.0, 1.0]\n",
      "1.0\t[[\"admiral\", \"multitude\"], 1.0, 1.0]\n",
      "1.0\t[[\"admire\", \"creativity\"], 1.0, 1.0]\n",
      "1.0\t[[\"admire\", \"expended\"], 1.0, 1.0]\n",
      "1.0\t[[\"admire\", \"private\"], 1.0, 1.0]\n",
      "0.025669642857142856\t[[\"may\", \"order\"], 0.03571428571428571, 0.015625]\n",
      "0.025667631214690755\t[[\"care\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025667631214690755\t[[\"come\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025667631214690755\t[[\"effect\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025667631214690755\t[[\"longer\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025667631214690755\t[[\"look\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025667631214690755\t[[\"south\", \"would\"], 0.03834824944236852, 0.012987012987012988]\n",
      "0.025377661365756775\t[[\"first\", \"time\"], 0.033806170189140665, 0.01694915254237288]\n",
      "0.024908171936628233\t[[\"could\", \"shall\"], 0.033149677206589796, 0.016666666666666666]\n",
      "0.02487578235111613\t[[\"attention\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"blood\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"children\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"give\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"interests\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"king\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"law\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"line\", \"one\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"one\", \"purpose\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"one\", \"taken\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"one\", \"things\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.02487578235111613\t[[\"one\", \"view\"], 0.037986858819879316, 0.011764705882352941]\n",
      "0.024692067013434675\t[[\"high\", \"would\"], 0.036563621206356534, 0.01282051282051282]\n",
      "0.024461722527284298\t[[\"could\", \"made\"], 0.03253000243161777, 0.01639344262295082]\n",
      "0.024461722527284298\t[[\"made\", \"time\"], 0.03253000243161777, 0.01639344262295082]\n",
      "0.024411633318294424\t[[\"large\", \"may\"], 0.033671751485073696, 0.015151515151515152]\n",
      "0.024411633318294424\t[[\"make\", \"may\"], 0.033671751485073696, 0.015151515151515152]\n",
      "0.024411633318294424\t[[\"may\", \"might\"], 0.033671751485073696, 0.015151515151515152]\n",
      "0.024411633318294424\t[[\"may\", \"take\"], 0.033671751485073696, 0.015151515151515152]\n",
      "0.02383270274228327\t[[\"done\", \"one\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.02383270274228327\t[[\"early\", \"one\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.02383270274228327\t[[\"god\", \"one\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.02383270274228327\t[[\"one\", \"south\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.02383270274228327\t[[\"one\", \"three\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.02383270274228327\t[[\"one\", \"went\"], 0.036037498507822355, 0.011627906976744186]\n",
      "0.023832614974400753\t[[\"action\", \"would\"], 0.03500700210070024, 0.012658227848101266]\n",
      "0.023832614974400753\t[[\"far\", \"would\"], 0.03500700210070024, 0.012658227848101266]\n",
      "0.023832614974400753\t[[\"general\", \"would\"], 0.03500700210070024, 0.012658227848101266]\n",
      "0.023832614974400753\t[[\"possible\", \"would\"], 0.03500700210070024, 0.012658227848101266]\n",
      "0.02363062845086655\t[[\"could\", \"two\"], 0.03138824102871723, 0.015873015873015872]\n",
      "0.02363062845086655\t[[\"could\", \"upon\"], 0.03138824102871723, 0.015873015873015872]\n",
      "0.02363062845086655\t[[\"could\", \"well\"], 0.03138824102871723, 0.015873015873015872]\n",
      "0.02363062845086655\t[[\"time\", \"two\"], 0.03138824102871723, 0.015873015873015872]\n",
      "0.02363062845086655\t[[\"time\", \"upon\"], 0.03138824102871723, 0.015873015873015872]\n",
      "0.023066819849907813\t[[\"said\", \"would\"], 0.03363363969981562, 0.0125]\n",
      "0.022927329755382846\t[[\"amount\", \"one\"], 0.034360406637202474, 0.011494252873563218]\n",
      "0.02237793259497695\t[[\"away\", \"would\"], 0.03241018617760822, 0.012345679012345678]\n",
      "0.02237793259497695\t[[\"every\", \"would\"], 0.03241018617760822, 0.012345679012345678]\n",
      "0.02237793259497695\t[[\"means\", \"would\"], 0.03241018617760822, 0.012345679012345678]\n",
      "0.022371479739686458\t[[\"many\", \"may\"], 0.03045724519365863, 0.014285714285714285]\n",
      "0.022371479739686458\t[[\"may\", \"way\"], 0.03045724519365863, 0.014285714285714285]\n",
      "0.02217493372557667\t[[\"part\", \"time\"], 0.029424494316824982, 0.014925373134328358]\n",
      "0.022130610555812405\t[[\"action\", \"one\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"day\", \"one\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"far\", \"one\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"general\", \"one\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"go\", \"one\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"one\", \"role\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"one\", \"see\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.022130610555812405\t[[\"one\", \"small\"], 0.03289758474798845, 0.011363636363636364]\n",
      "0.021936140239489244\t[[\"long\", \"may\"], 0.029787773436724965, 0.014084507042253521]\n",
      "0.02152474053243955\t[[\"great\", \"may\"], 0.029160592175990215, 0.013888888888888888]\n",
      "0.021421466059115237\t[[\"development\", \"one\"], 0.0316069770620507, 0.011235955056179775]\n",
      "0.021182572950312978\t[[\"back\", \"would\"], 0.03031695312954162, 0.012048192771084338]\n",
      "0.021182572950312978\t[[\"left\", \"would\"], 0.03031695312954162, 0.012048192771084338]\n",
      "0.021182572950312978\t[[\"power\", \"would\"], 0.03031695312954162, 0.012048192771084338]\n",
      "0.021135029354207434\t[[\"first\", \"may\"], 0.02857142857142857, 0.0136986301369863]\n",
      "0.021135029354207434\t[[\"may\", \"must\"], 0.02857142857142857, 0.0136986301369863]\n",
      "0.020784178152384872\t[[\"every\", \"one\"], 0.03045724519365863, 0.011111111111111112]\n",
      "0.020784178152384872\t[[\"life\", \"one\"], 0.03045724519365863, 0.011111111111111112]\n",
      "0.020784178152384872\t[[\"never\", \"one\"], 0.03045724519365863, 0.011111111111111112]\n",
      "0.020784178152384872\t[[\"one\", \"people\"], 0.03045724519365863, 0.011111111111111112]\n",
      "0.02041310164737204\t[[\"made\", \"may\"], 0.02749286996141075, 0.013333333333333334]\n",
      "0.020206752652917985\t[[\"light\", \"one\"], 0.029424494316824982, 0.01098901098901099]\n",
      "0.020206752652917985\t[[\"one\", \"sense\"], 0.029424494316824982, 0.01098901098901099]\n",
      "0.020173901703052208\t[[\"large\", \"would\"], 0.028583097523751475, 0.011764705882352941]\n",
      "0.020173901703052208\t[[\"make\", \"would\"], 0.028583097523751475, 0.011764705882352941]\n",
      "0.020173901703052208\t[[\"men\", \"would\"], 0.028583097523751475, 0.011764705882352941]\n",
      "0.020173901703052208\t[[\"system\", \"would\"], 0.028583097523751475, 0.011764705882352941]\n",
      "0.020173901703052208\t[[\"used\", \"would\"], 0.028583097523751475, 0.011764705882352941]\n",
      "0.01975745922043877\t[[\"may\", \"well\"], 0.02652790545386455, 0.012987012987012988]\n",
      "0.019679854666150395\t[[\"end\", \"one\"], 0.028490144114909487, 0.010869565217391304]\n",
      "0.019679854666150395\t[[\"left\", \"one\"], 0.028490144114909487, 0.010869565217391304]\n",
      "0.019679854666150395\t[[\"one\", \"order\"], 0.028490144114909487, 0.010869565217391304]\n",
      "0.019679854666150395\t[[\"one\", \"power\"], 0.028490144114909487, 0.010869565217391304]\n",
      "0.019679854666150395\t[[\"one\", \"world\"], 0.028490144114909487, 0.010869565217391304]\n",
      "0.019196093906591057\t[[\"also\", \"one\"], 0.027639499641139105, 0.010752688172043012]\n",
      "0.019196093906591057\t[[\"good\", \"one\"], 0.027639499641139105, 0.010752688172043012]\n",
      "0.019196093906591057\t[[\"one\", \"right\"], 0.027639499641139105, 0.010752688172043012]\n",
      "0.018913221282442258\t[[\"without\", \"would\"], 0.026462806201248155, 0.011363636363636364]\n",
      "0.01874953166992655\t[[\"one\", \"state\"], 0.026860765467512676, 0.010638297872340425]\n",
      "0.01874953166992655\t[[\"one\", \"system\"], 0.026860765467512676, 0.010638297872340425]\n",
      "0.018606957788722685\t[[\"may\", \"part\"], 0.02486823656509969, 0.012345679012345678]\n",
      "0.017871349781713924\t[[\"number\", \"would\"], 0.02475368857441686, 0.01098901098901099]\n",
      "0.017871349781713924\t[[\"us\", \"would\"], 0.02475368857441686, 0.01098901098901099]\n",
      "0.017588757457807577\t[[\"one\", \"without\"], 0.02486823656509969, 0.010309278350515464]\n",
      "0.017561563860512303\t[[\"first\", \"would\"], 0.024253562503633298, 0.010869565217391304]\n",
      "0.01698814963640363\t[[\"man\", \"would\"], 0.02333800140046683, 0.010638297872340425]\n",
      "0.016076714691005578\t[[\"one\", \"shall\"], 0.02234950781338371, 0.00980392156862745]\n",
      "0.015820230514701654\t[[\"made\", \"one\"], 0.021931723165325635, 0.009708737864077669]\n",
      "0.015820230514701654\t[[\"much\", \"one\"], 0.021931723165325635, 0.009708737864077669]\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 test_5.7.1_similarity_test_1.txt\n",
    "!tail -n 100 test_5.7.1_similarity_test_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.2 Running on 20 test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 06:27:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/5.7.1_test_20' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/filterStopWords.daghan.20180308.062707.947050\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/filterStopWords.daghan.20180308.062707.947050/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4653234943039174778.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30533\n",
      "  Submitted application application_1509050304403_30533\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30533/\n",
      "  Running job: job_1509050304403_30533\n",
      "  Job job_1509050304403_30533 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30533 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_test_20\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1784506\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1732077\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1231017\n",
      "\t\tFILE: Number of bytes written=2859313\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1784856\n",
      "\t\tHDFS: Number of bytes written=1732077\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25161216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=55362560\n",
      "\t\tTotal time spent by all map tasks (ms)=16381\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49143\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21626\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=108130\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16381\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=21626\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=14450\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=665\n",
      "\t\tInput split bytes=350\n",
      "\t\tMap input records=114101\n",
      "\t\tMap output bytes=1846051\n",
      "\t\tMap output materialized bytes=1231220\n",
      "\t\tMap output records=113974\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1920225280\n",
      "\t\tReduce input groups=113974\n",
      "\t\tReduce input records=113974\n",
      "\t\tReduce output records=113974\n",
      "\t\tReduce shuffle bytes=1231220\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=227948\n",
      "\t\tTotal committed heap usage (bytes)=2238709760\n",
      "\t\tVirtual memory (bytes) snapshot=11413606400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/filterStopWords.daghan.20180308.062707.947050...\n",
      "Removing temp directory /tmp/filterStopWords.daghan.20180308.062707.947050...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python filterStopWords.py -r  hadoop test_20.B.txt \\\n",
    "    --output-dir={OUTPUT_PATH} --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1.test_20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10000 test_5.7.1.test_20.txt > test_5.7.1.test_20_top_10K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/08 06:32:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/hw5/5.7.1_stripes_test_20' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/daghan/.Trash/Current\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes2.daghan.20180308.063253.246398\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/buildStripes2.daghan.20180308.063253.246398/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob665503978187780051.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1509050304403_30552\n",
      "  Submitted application application_1509050304403_30552\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30552/\n",
      "  Running job: job_1509050304403_30552\n",
      "  Job job_1509050304403_30552 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30552 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_stripes_test_20\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=217918535\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1939820\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1978342\n",
      "\t\tFILE: Number of bytes written=7342117\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217921825\n",
      "\t\tHDFS: Number of bytes written=1939820\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=63\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=14\n",
      "\t\tLaunched map tasks=34\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=33\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17922230784\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=35489280\n",
      "\t\tTotal time spent by all map tasks (ms)=11668119\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35004357\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13863\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=69315\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11668119\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13863\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5503090\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9562\n",
      "\t\tInput split bytes=3290\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=4355494\n",
      "\t\tMap output materialized bytes=2595159\n",
      "\t\tMap output records=164574\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tPhysical memory (bytes) snapshot=16604520448\n",
      "\t\tReduce input groups=9327\n",
      "\t\tReduce input records=164574\n",
      "\t\tReduce output records=9327\n",
      "\t\tReduce shuffle bytes=2595159\n",
      "\t\tShuffled Maps =20\n",
      "\t\tSpilled Records=329148\n",
      "\t\tTotal committed heap usage (bytes)=18102616064\n",
      "\t\tVirtual memory (bytes) snapshot=73035145216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/buildStripes2.daghan.20180308.063253.246398...\n",
      "Removing temp directory /tmp/buildStripes2.daghan.20180308.063253.246398...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_stripes_test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python buildStripes2.py hdfs://{TEST_20} \\\n",
    "    -r hadoop --file test_5.7.1.test_20_top_10K.txt \\\n",
    "    --output-dir={OUTPUT_PATH} --no-output    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_stripes_test_20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/daghan/hw5/5.7.1_inverted_test_20': No such file or directory\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex.daghan.20180308.064151.446356\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180308.064151.446356/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3164544709666941814.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30580\n",
      "  Submitted application application_1509050304403_30580\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30580/\n",
      "  Running job: job_1509050304403_30580\n",
      "  Job job_1509050304403_30580 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30580 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_inverted_test_20\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2018486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1857111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1195871\n",
      "\t\tFILE: Number of bytes written=2757964\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2018868\n",
      "\t\tHDFS: Number of bytes written=1857111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=71347200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=42767360\n",
      "\t\tTotal time spent by all map tasks (ms)=46450\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=139350\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16706\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=83530\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=46450\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16706\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=716\n",
      "\t\tInput split bytes=382\n",
      "\t\tMap input records=9327\n",
      "\t\tMap output bytes=3086727\n",
      "\t\tMap output materialized bytes=1166535\n",
      "\t\tMap output records=113293\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1886203904\n",
      "\t\tReduce input groups=113293\n",
      "\t\tReduce input records=113293\n",
      "\t\tReduce output records=1000\n",
      "\t\tReduce shuffle bytes=1166535\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=226586\n",
      "\t\tTotal committed heap usage (bytes)=2254962688\n",
      "\t\tVirtual memory (bytes) snapshot=11408969728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/invertedIndex.daghan.20180308.064151.446356...\n",
      "Removing temp directory /tmp/invertedIndex.daghan.20180308.064151.446356...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_inverted_test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python invertedIndex.py test_5.7.1_stripes_test_20.txt -r hadoop \\\n",
    "    --output-dir={OUTPUT_PATH} --no-output    \n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_inverted_test_20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/daghan/hw5/5.7.1_similarity_test_20': No such file or directory\n",
      "Using configs in /home/daghan/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity.daghan.20180308.064511.868446\n",
      "Copying local files to hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.064511.868446/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob772381827842137097.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30592\n",
      "  Submitted application application_1509050304403_30592\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30592/\n",
      "  Running job: job_1509050304403_30592\n",
      "  Job job_1509050304403_30592 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30592 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.064511.868446/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1977132\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=350912717\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=53328196\n",
      "\t\tFILE: Number of bytes written=114589004\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1977510\n",
      "\t\tHDFS: Number of bytes written=350912717\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=264599040\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1086891520\n",
      "\t\tTotal time spent by all map tasks (ms)=172265\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=516795\n",
      "\t\tTotal time spent by all reduce tasks (ms)=424567\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2122835\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=172265\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=424567\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=570300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=576\n",
      "\t\tInput split bytes=378\n",
      "\t\tMap input records=1000\n",
      "\t\tMap output bytes=240435681\n",
      "\t\tMap output materialized bytes=60865253\n",
      "\t\tMap output records=6950024\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=2243420160\n",
      "\t\tReduce input groups=4097390\n",
      "\t\tReduce input records=6950024\n",
      "\t\tReduce output records=4097390\n",
      "\t\tReduce shuffle bytes=60865253\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=13900048\n",
      "\t\tTotal committed heap usage (bytes)=2310537216\n",
      "\t\tVirtual memory (bytes) snapshot=11415220224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1331154004714235495.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  Adding a new node: /default-rack/10.251.253.174:50010\n",
      "  Adding a new node: /default-rack/10.251.240.106:50010\n",
      "  Adding a new node: /default-rack/10.251.249.182:50010\n",
      "  Adding a new node: /default-rack/10.251.233.203:50010\n",
      "  Adding a new node: /default-rack/10.251.235.85:50010\n",
      "  Adding a new node: /default-rack/10.251.235.73:50010\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_30619\n",
      "  Submitted application application_1509050304403_30619\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_30619/\n",
      "  Running job: job_1509050304403_30619\n",
      "  Job job_1509050304403_30619 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_30619 completed successfully\n",
      "  Output directory: hdfs:///user/daghan/hw5/5.7.1_similarity_test_20\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=350961767\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=347938641\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52887424\n",
      "\t\tFILE: Number of bytes written=108991216\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=350962123\n",
      "\t\tHDFS: Number of bytes written=347938641\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=168975360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=430092800\n",
      "\t\tTotal time spent by all map tasks (ms)=110010\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=330030\n",
      "\t\tTotal time spent by all reduce tasks (ms)=168005\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=840025\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=110010\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=168005\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=285440\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1799\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=4097390\n",
      "\t\tMap output bytes=355010107\n",
      "\t\tMap output materialized bytes=55707226\n",
      "\t\tMap output records=4097390\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=2438402048\n",
      "\t\tReduce input groups=4097390\n",
      "\t\tReduce input records=4097390\n",
      "\t\tReduce output records=4097390\n",
      "\t\tReduce shuffle bytes=55707226\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=8194780\n",
      "\t\tTotal committed heap usage (bytes)=2717908992\n",
      "\t\tVirtual memory (bytes) snapshot=11422449664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/daghan/tmp/mrjob/similarity.daghan.20180308.064511.868446...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/similarity.daghan.20180308.064511.868446...\r\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/user/daghan/hw5/5.7.1_similarity_test_20\"\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python similarity.py test_5.7.1_inverted_test_20.txt -r hadoop \\\n",
    "    --output-dir={OUTPUT_PATH} --no-output\n",
    "!hdfs dfs -cat {OUTPUT_PATH}/part* > test_5.7.1_similarity_test_20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[[\"abilities\", \"footing\"], 1.0, 1.0]\n",
      "1.0\t[[\"abilities\", \"hired\"], 1.0, 1.0]\n",
      "1.0\t[[\"abreast\", \"superseded\"], 1.0, 1.0]\n",
      "1.0\t[[\"abreast\", \"tracks\"], 1.0, 1.0]\n",
      "1.0\t[[\"abreast\", \"versions\"], 1.0, 1.0]\n",
      "1.0\t[[\"absorb\", \"blanket\"], 1.0, 1.0]\n",
      "1.0\t[[\"abstain\", \"dismiss\"], 1.0, 1.0]\n",
      "1.0\t[[\"abstain\", \"enrolled\"], 1.0, 1.0]\n",
      "1.0\t[[\"abstain\", \"surrendered\"], 1.0, 1.0]\n",
      "1.0\t[[\"abstain\", \"thereto\"], 1.0, 1.0]\n",
      "1.0\t[[\"accent\", \"poet's\"], 1.0, 1.0]\n",
      "1.0\t[[\"accent\", \"prominence\"], 1.0, 1.0]\n",
      "1.0\t[[\"accent\", \"trait\"], 1.0, 1.0]\n",
      "1.0\t[[\"accepts\", \"storage\"], 1.0, 1.0]\n",
      "1.0\t[[\"accessible\", \"denoted\"], 1.0, 1.0]\n",
      "1.0\t[[\"accessible\", \"interfering\"], 1.0, 1.0]\n",
      "1.0\t[[\"accommodate\", \"instruct\"], 1.0, 1.0]\n",
      "1.0\t[[\"accomplish\", \"catalyst\"], 1.0, 1.0]\n",
      "1.0\t[[\"accorded\", \"gotten\"], 1.0, 1.0]\n",
      "1.0\t[[\"accountable\", \"adopting\"], 1.0, 1.0]\n",
      "1.0\t[[\"accountable\", \"tracing\"], 1.0, 1.0]\n",
      "1.0\t[[\"accounting\", \"confidential\"], 1.0, 1.0]\n",
      "1.0\t[[\"acre\", \"atlanta\"], 1.0, 1.0]\n",
      "1.0\t[[\"acutely\", \"pierced\"], 1.0, 1.0]\n",
      "1.0\t[[\"acutely\", \"rounds\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressing\", \"chariot\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressing\", \"cigar\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressing\", \"pillar\"], 1.0, 1.0]\n",
      "1.0\t[[\"addressing\", \"tongues\"], 1.0, 1.0]\n",
      "1.0\t[[\"adhere\", \"practiced\"], 1.0, 1.0]\n",
      "1.0\t[[\"adhere\", \"receptive\"], 1.0, 1.0]\n",
      "1.0\t[[\"adhere\", \"reject\"], 1.0, 1.0]\n",
      "1.0\t[[\"adhered\", \"linguistics\"], 1.0, 1.0]\n",
      "1.0\t[[\"adherents\", \"medieval\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjunct\", \"creditors\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjunct\", \"exhaust\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjunct\", \"prescribe\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjusting\", \"shorten\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"dictate\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"finishing\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"foster\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"knot\"], 1.0, 1.0]\n",
      "1.0\t[[\"adjustments\", \"optimum\"], 1.0, 1.0]\n",
      "1.0\t[[\"administrators\", \"doubting\"], 1.0, 1.0]\n",
      "1.0\t[[\"admissible\", \"definitive\"], 1.0, 1.0]\n",
      "1.0\t[[\"adopting\", \"tracing\"], 1.0, 1.0]\n",
      "1.0\t[[\"advantageous\", \"benefited\"], 1.0, 1.0]\n",
      "1.0\t[[\"advantageous\", \"sectors\"], 1.0, 1.0]\n",
      "1.0\t[[\"advantageous\", \"utilized\"], 1.0, 1.0]\n",
      "1.0\t[[\"advantageous\", \"weaker\"], 1.0, 1.0]\n",
      "1.0\t[[\"advise\", \"brighter\"], 1.0, 1.0]\n",
      "1.0\t[[\"advise\", \"exhibited\"], 1.0, 1.0]\n",
      "1.0\t[[\"advise\", \"fantastic\"], 1.0, 1.0]\n",
      "1.0\t[[\"advise\", \"upset\"], 1.0, 1.0]\n",
      "1.0\t[[\"adviser\", \"criminology\"], 1.0, 1.0]\n",
      "1.0\t[[\"adviser\", \"jones\"], 1.0, 1.0]\n",
      "1.0\t[[\"adviser\", \"reprinted\"], 1.0, 1.0]\n",
      "1.0\t[[\"advisory\", \"allocated\"], 1.0, 1.0]\n",
      "1.0\t[[\"advisory\", \"implement\"], 1.0, 1.0]\n",
      "1.0\t[[\"advocated\", \"physicians\"], 1.0, 1.0]\n",
      "1.0\t[[\"affirmation\", \"benevolence\"], 1.0, 1.0]\n",
      "1.0\t[[\"agitation\", \"comprehension\"], 1.0, 1.0]\n",
      "1.0\t[[\"agony\", \"frustration\"], 1.0, 1.0]\n",
      "1.0\t[[\"aldermen\", \"deaths\"], 1.0, 1.0]\n",
      "1.0\t[[\"aldermen\", \"fringe\"], 1.0, 1.0]\n",
      "1.0\t[[\"allocated\", \"implement\"], 1.0, 1.0]\n",
      "1.0\t[[\"allowances\", \"cholesterol\"], 1.0, 1.0]\n",
      "1.0\t[[\"allowances\", \"restrictions\"], 1.0, 1.0]\n",
      "1.0\t[[\"almighty\", \"believer\"], 1.0, 1.0]\n",
      "1.0\t[[\"almighty\", \"marking\"], 1.0, 1.0]\n",
      "1.0\t[[\"almighty\", \"meditation\"], 1.0, 1.0]\n",
      "1.0\t[[\"almighty\", \"scientist\"], 1.0, 1.0]\n",
      "1.0\t[[\"amazed\", \"samuel\"], 1.0, 1.0]\n",
      "1.0\t[[\"amazing\", \"unlimited\"], 1.0, 1.0]\n",
      "1.0\t[[\"ambiguity\", \"disturbances\"], 1.0, 1.0]\n",
      "1.0\t[[\"ambiguity\", \"interpretations\"], 1.0, 1.0]\n",
      "1.0\t[[\"ambiguity\", \"remission\"], 1.0, 1.0]\n",
      "1.0\t[[\"amend\", \"permanence\"], 1.0, 1.0]\n",
      "1.0\t[[\"amplifier\", \"publicity\"], 1.0, 1.0]\n",
      "1.0\t[[\"amply\", \"files\"], 1.0, 1.0]\n",
      "1.0\t[[\"analogy\", \"disciplines\"], 1.0, 1.0]\n",
      "1.0\t[[\"analytical\", \"conceptual\"], 1.0, 1.0]\n",
      "1.0\t[[\"analytical\", \"costume\"], 1.0, 1.0]\n",
      "1.0\t[[\"analytical\", \"delicacy\"], 1.0, 1.0]\n",
      "1.0\t[[\"anarchy\", \"tore\"], 1.0, 1.0]\n",
      "1.0\t[[\"anarchy\", \"withstand\"], 1.0, 1.0]\n",
      "1.0\t[[\"annals\", \"ascended\"], 1.0, 1.0]\n",
      "1.0\t[[\"anne\", \"denmark\"], 1.0, 1.0]\n",
      "1.0\t[[\"anne\", \"fuller\"], 1.0, 1.0]\n",
      "1.0\t[[\"anne\", \"usa\"], 1.0, 1.0]\n",
      "1.0\t[[\"annum\", \"billion\"], 1.0, 1.0]\n",
      "1.0\t[[\"annum\", \"invested\"], 1.0, 1.0]\n",
      "1.0\t[[\"anonymous\", \"prejudices\"], 1.0, 1.0]\n",
      "1.0\t[[\"antecedent\", \"expertise\"], 1.0, 1.0]\n",
      "1.0\t[[\"antecedent\", \"revealing\"], 1.0, 1.0]\n",
      "1.0\t[[\"apex\", \"devote\"], 1.0, 1.0]\n",
      "1.0\t[[\"apex\", \"irrespective\"], 1.0, 1.0]\n",
      "1.0\t[[\"apex\", \"turmoil\"], 1.0, 1.0]\n",
      "1.0\t[[\"applicant\", \"seller\"], 1.0, 1.0]\n",
      "1.0\t[[\"appraisal\", \"severed\"], 1.0, 1.0]\n",
      "0.008176526206641839\t[[\"contraction\", \"one\"], 0.01465525954401373, 0.001697792869269949]\n",
      "0.008176526206641839\t[[\"describe\", \"one\"], 0.01465525954401373, 0.001697792869269949]\n",
      "0.008176526206641839\t[[\"inch\", \"one\"], 0.01465525954401373, 0.001697792869269949]\n",
      "0.008176526206641839\t[[\"manifestation\", \"one\"], 0.01465525954401373, 0.001697792869269949]\n",
      "0.008176526206641839\t[[\"one\", \"sciences\"], 0.01465525954401373, 0.001697792869269949]\n",
      "0.008176491239963444\t[[\"nuclear\", \"upon\"], 0.01332267944962386, 0.0030303030303030303]\n",
      "0.008176491239963444\t[[\"project\", \"upon\"], 0.01332267944962386, 0.0030303030303030303]\n",
      "0.008176491239963444\t[[\"serum\", \"upon\"], 0.01332267944962386, 0.0030303030303030303]\n",
      "0.008172113846588459\t[[\"man\", \"specific\"], 0.012527433800047148, 0.003816793893129771]\n",
      "0.008170023539334454\t[[\"could\", \"cultural\"], 0.013017787942456283, 0.0033222591362126247]\n",
      "0.008159123096242697\t[[\"back\", \"considered\"], 0.011163607017227663, 0.005154639175257732]\n",
      "0.008156525148466721\t[[\"day\", \"function\"], 0.0110498924021966, 0.005263157894736842]\n",
      "0.008146183037132174\t[[\"anger\", \"well\"], 0.013280317881493264, 0.0030120481927710845]\n",
      "0.008146183037132174\t[[\"sitting\", \"well\"], 0.013280317881493264, 0.0030120481927710845]\n",
      "0.008146183037132174\t[[\"wealth\", \"well\"], 0.013280317881493264, 0.0030120481927710845]\n",
      "0.008118390203149178\t[[\"rapid\", \"shall\"], 0.01207011373963169, 0.004166666666666667]\n",
      "0.008111073177607448\t[[\"couple\", \"much\"], 0.012448561449554519, 0.0037735849056603774]\n",
      "0.008111073177607448\t[[\"much\", \"seven\"], 0.012448561449554519, 0.0037735849056603774]\n",
      "0.008094341362874624\t[[\"around\", \"character\"], 0.01078327732034384, 0.005405405405405406]\n",
      "0.008079046788724209\t[[\"political\", \"went\"], 0.010752688172043012, 0.005405405405405406]\n",
      "0.008073119202460722\t[[\"met\", \"must\"], 0.01254911610276317, 0.0035971223021582736]\n",
      "0.008073119202460722\t[[\"must\", \"sexual\"], 0.01254911610276317, 0.0035971223021582736]\n",
      "0.008073110668538845\t[[\"may\", \"plains\"], 0.014027577269281081, 0.00211864406779661]\n",
      "0.008073110668538845\t[[\"may\", \"province\"], 0.014027577269281081, 0.00211864406779661]\n",
      "0.008073087518745123\t[[\"means\", \"told\"], 0.011495012246792571, 0.004651162790697674]\n",
      "0.008041532887662187\t[[\"bay\", \"would\"], 0.013840913308956662, 0.002242152466367713]\n",
      "0.008041532887662187\t[[\"brief\", \"would\"], 0.013840913308956662, 0.002242152466367713]\n",
      "0.008041532887662187\t[[\"glow\", \"would\"], 0.013840913308956662, 0.002242152466367713]\n",
      "0.008041532887662187\t[[\"lesions\", \"would\"], 0.013840913308956662, 0.002242152466367713]\n",
      "0.008041532887662187\t[[\"posterior\", \"would\"], 0.013840913308956662, 0.002242152466367713]\n",
      "0.0080405563468015\t[[\"back\", \"changes\"], 0.01097907187727647, 0.00510204081632653]\n",
      "0.0080405563468015\t[[\"back\", \"greater\"], 0.01097907187727647, 0.00510204081632653]\n",
      "0.008040436966268391\t[[\"data\", \"old\"], 0.011576369428032278, 0.0045045045045045045]\n",
      "0.008039359294615476\t[[\"great\", \"release\"], 0.013029938101426072, 0.003048780487804878]\n",
      "0.008014873359827713\t[[\"could\", \"skin\"], 0.01271848844150973, 0.0033112582781456954]\n",
      "0.008014500804020323\t[[\"coming\", \"form\"], 0.01154469667530522, 0.004484304932735426]\n",
      "0.008006740777830838\t[[\"bill\", \"two\"], 0.012888481555661675, 0.003125]\n",
      "0.007994245998889652\t[[\"chronic\", \"upon\"], 0.012967343961525527, 0.0030211480362537764]\n",
      "0.007994245998889652\t[[\"issues\", \"upon\"], 0.012967343961525527, 0.0030211480362537764]\n",
      "0.00798883119508559\t[[\"membrane\", \"time\"], 0.013386988815041648, 0.0025906735751295338]\n",
      "0.00798793098525036\t[[\"acute\", \"way\"], 0.012515654358043972, 0.0034602076124567475]\n",
      "0.0079780389526418\t[[\"positive\", \"us\"], 0.012139284012153828, 0.003816793893129771]\n",
      "0.007970235339333328\t[[\"corner\", \"many\"], 0.01249219481659769, 0.0034482758620689655]\n",
      "0.007964557620913845\t[[\"violent\", \"well\"], 0.012926112238824687, 0.003003003003003003]\n",
      "0.007964385746359479\t[[\"central\", \"said\"], 0.011189434999827962, 0.004739336492890996]\n",
      "0.007928593332066157\t[[\"death\", \"group\"], 0.010594028769395471, 0.005263157894736842]\n",
      "0.007925585060020796\t[[\"patients\", \"still\"], 0.010973121339553788, 0.004878048780487805]\n",
      "0.00788599246337871\t[[\"use\", \"wind\"], 0.011723401930806003, 0.004048582995951417]\n",
      "0.007881298763066896\t[[\"long\", \"produce\"], 0.011664236870396087, 0.004098360655737705]\n",
      "0.00786604177192914\t[[\"based\", \"far\"], 0.010901165669462145, 0.004830917874396135]\n",
      "0.00786604177192914\t[[\"came\", \"cases\"], 0.010901165669462145, 0.004830917874396135]\n",
      "0.007823410585732772\t[[\"carrying\", \"must\"], 0.012075392600036974, 0.0035714285714285713]\n",
      "0.007820573577279643\t[[\"first\", \"release\"], 0.012725695259515555, 0.0029154518950437317]\n",
      "0.00780764810834648\t[[\"patients\", \"work\"], 0.010830607221477648, 0.004784688995215311]\n",
      "0.00778240590403621\t[[\"new\", \"stream\"], 0.012242552671859794, 0.0033222591362126247]\n",
      "0.0077729377561478455\t[[\"muscle\", \"time\"], 0.012961896184130316, 0.002583979328165375]\n",
      "0.0077722738193094585\t[[\"castle\", \"may\"], 0.013430382733756338, 0.0021141649048625794]\n",
      "0.0077722738193094585\t[[\"expansion\", \"may\"], 0.013430382733756338, 0.0021141649048625794]\n",
      "0.007767527918080662\t[[\"tube\", \"would\"], 0.013297919370836938, 0.0022371364653243847]\n",
      "0.007756013229532629\t[[\"disorder\", \"one\"], 0.01381711120482797, 0.001694915254237288]\n",
      "0.007756013229532629\t[[\"kidney\", \"one\"], 0.01381711120482797, 0.001694915254237288]\n",
      "0.007756013229532629\t[[\"one\", \"tissues\"], 0.01381711120482797, 0.001694915254237288]\n",
      "0.007748689735213268\t[[\"energy\", \"shall\"], 0.01139901881468883, 0.004098360655737705]\n",
      "0.007733249643898802\t[[\"acute\", \"could\"], 0.012177025603587077, 0.003289473684210526]\n",
      "0.0077048242817126505\t[[\"like\", \"test\"], 0.011344607913018797, 0.0040650406504065045]\n",
      "0.0077021759660415474\t[[\"know\", \"within\"], 0.010478243557698858, 0.0049261083743842365]\n",
      "0.00766739646121788\t[[\"number\", \"southern\"], 0.011503375297914687, 0.0038314176245210726]\n",
      "0.007597056684762775\t[[\"day\", \"effects\"], 0.010218988991416097, 0.004975124378109453]\n",
      "0.0075650675143329234\t[[\"great\", \"processes\"], 0.01210898699241207, 0.0030211480362537764]\n",
      "0.0075650675143329234\t[[\"great\", \"weight\"], 0.01210898699241207, 0.0030211480362537764]\n",
      "0.007542606523853702\t[[\"major\", \"see\"], 0.010300524052492094, 0.004784688995215311]\n",
      "0.007523169298892175\t[[\"chemical\", \"would\"], 0.012814195740641491, 0.002232142857142857]\n",
      "0.007523169298892175\t[[\"density\", \"would\"], 0.012814195740641491, 0.002232142857142857]\n",
      "0.007523169298892175\t[[\"index\", \"would\"], 0.012814195740641491, 0.002232142857142857]\n",
      "0.0075224144793758965\t[[\"diagnosis\", \"upon\"], 0.012050816982703888, 0.0029940119760479044]\n",
      "0.007520626942739816\t[[\"brought\", \"development\"], 0.010041253885479632, 0.005]\n",
      "0.0074782657926659\t[[\"lack\", \"us\"], 0.011211213233271875, 0.003745318352059925]\n",
      "0.007463479384413866\t[[\"like\", \"principle\"], 0.01091089451179962, 0.004016064257028112]\n",
      "0.007440753938384255\t[[\"might\", \"structure\"], 0.010495542964487809, 0.0043859649122807015]\n",
      "0.007400055002326736\t[[\"chicago\", \"one\"], 0.013108062627326908, 0.001692047377326565]\n",
      "0.007400055002326736\t[[\"one\", \"pulmonary\"], 0.013108062627326908, 0.001692047377326565]\n",
      "0.007395643502924853\t[[\"basic\", \"time\"], 0.012220592918446108, 0.002570694087403599]\n",
      "0.007395643502924853\t[[\"serum\", \"time\"], 0.012220592918446108, 0.002570694087403599]\n",
      "0.007385600810794145\t[[\"associated\", \"whole\"], 0.010076365940837117, 0.004694835680751174]\n",
      "0.007368969414433095\t[[\"could\", \"groups\"], 0.011480609838638177, 0.003257328990228013]\n",
      "0.007303430352004179\t[[\"membrane\", \"would\"], 0.012379689211803458, 0.0022271714922048997]\n",
      "0.007221512114047381\t[[\"associated\", \"came\"], 0.009876814182432662, 0.0045662100456621]\n",
      "0.0071654188285374685\t[[\"till\", \"use\"], 0.010424587657074937, 0.00390625]\n",
      "0.007104402379678413\t[[\"channel\", \"would\"], 0.011986582537134603, 0.0022222222222222222]\n",
      "0.007104402379678413\t[[\"liver\", \"would\"], 0.011986582537134603, 0.0022222222222222222]\n",
      "0.007104402379678413\t[[\"muscle\", \"would\"], 0.011986582537134603, 0.0022222222222222222]\n",
      "0.0070755085755187285\t[[\"concentration\", \"time\"], 0.011593472394004208, 0.0025575447570332483]\n",
      "0.00700221952276296\t[[\"put\", \"study\"], 0.009331541849264238, 0.004672897196261682]\n",
      "0.006953310536294846\t[[\"great\", \"protein\"], 0.010930430596399216, 0.002976190476190476]\n",
      "0.006863744344340548\t[[\"may\", \"solemn\"], 0.011631052629980886, 0.0020964360587002098]\n",
      "0.006862006525519585\t[[\"get\", \"nature\"], 0.009279568606594726, 0.0044444444444444444]\n",
      "0.006826154975552539\t[[\"anterior\", \"one\"], 0.011965969310295635, 0.0016863406408094434]\n",
      "0.006826154975552539\t[[\"cardiac\", \"one\"], 0.011965969310295635, 0.0016863406408094434]\n",
      "0.006675542694346077\t[[\"next\", \"power\"], 0.009166984970282113, 0.0041841004184100415]\n",
      "0.006439023957387874\t[[\"produced\", \"shall\"], 0.009104463009115372, 0.0037735849056603774]\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 test_5.7.1_similarity_test_20.txt\n",
    "!tail -n 100 test_5.7.1_similarity_test_20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.3 Running the full dataset on Altiscale\n",
    "\n",
    "Please contact the TAs for approval after obtaining results from 5.7.2. We have ran into issues in the past where the clusters froze because people did not test their code on a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADD CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty print results\n",
    "NOTE: depending on how you processed the stop words your results may differ from the table provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print ''*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print ''*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.8 - Evaluation of synonyms that your discovered\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector. Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.7, and use the synonyms function from the wordnet synonnyms list from the nltk package (see provided code below).\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.9 - OPTIONAL: using different vocabulary subsets\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.10  - OPTIONAL \n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.11 - OPTIONAL \n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.7), this time for your\n",
    "results from 5.8. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1080px",
    "left": "0px",
    "right": "1300px",
    "top": "107px",
    "width": "318px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
